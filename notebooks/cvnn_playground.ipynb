{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVNN Playground\n",
    "\n",
    "The purpose of this notebook is to provide a simple playground for the Complex-Valued Neural Networks (CVNN) implementation, in the aim of proving their strengths vs real-valued neural networks. The CVNN is a neural network that uses complex numbers as input, output, and weights. The CVNN is implemented using PyTorch and the complextorch library.  \n",
    "  \n",
    "The notebook calls to a couple of other python files:  \n",
    "- `cvnn_data.py` contains the data generation functions.  \n",
    "- `cvnn_model.py` contains the CVNN model definitions.  \n",
    "\n",
    "The notebook is divided into the following sections:\n",
    "1. **Imports and settings** - Importing the necessary libraries and setting the data paths etc.\n",
    "2. **Data Preparation** - Generating the data for the CVNN. By default this adds noise to the data, randomly phase shifts the data by either -pi, 0, or pi, and randomly shifts the signal in time by a value between -0.1s and 0.1s.\n",
    "3. **Training loop definition**.\n",
    "4. **Complex Valued Training** - Training `ComplexValuedNN` defined in `cvnn_models.py` on the generated data. This model uses two 1d complex convolutional layers with fully complex activation functions (Cardoid) and two fully connected layers.  \n",
    "5. **Real Valued Training** - Training `RealValuedNN` defined in `cvnn_models.py` on the generated data. This model uses two 1d real convolutional layers (real and imag in a channel each) with ReLU activation functions and two fully connected layers.\n",
    "\n",
    "### Results:  \n",
    "Conv layers are required? \n",
    "It seems CVNNs can learn better than RVNNs when turning the number of conv filters down so the network goes <1m parameters.  \n",
    "\n",
    "Plot of the loss and accuracy of the CVNN and RVNN models during training:\n",
    "![lossoutput_1m_pars.png](../cvnn_lossoutput_1m_pars.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.fft import rfft, irfft\n",
    "\n",
    "from functools import partial\n",
    "rfft = partial(rfft, norm='ortho')\n",
    "irfft = partial(irfft, norm='ortho')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Sampler\n",
    "from typing import Iterator, Sized, List\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from cvnn_models import ComplexValuedNN, RealValuedNN\n",
    "from cvnn_data import get_data, GlitchDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from antiglitch import SnippetNormed\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "# Torch setup\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "def memsum():\n",
    "    \"\"\"Prints a summary of the GPU memory usage.\"\"\"\n",
    "    print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switches to control training\n",
    "train_complex, test_complex = True, True\n",
    "train_real, test_real = True, True\n",
    "train_real_td, test_real_td = False, False\n",
    "\n",
    "# define dataset\n",
    "NTRAIN = 200000\n",
    "NTEST = 50000\n",
    "\n",
    "rootdir = '/home/xangma/OneDrive/repos/antiglitch/'\n",
    "datadir = rootdir + 'data/'\n",
    "\n",
    "# set seed for data generation - noise, augmentation, etc.\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xangma/OneDrive/repos/antiglitch/antiglitch/utils.py:29: RuntimeWarning: divide by zero encountered in power\n",
      "  invasd = ((4096.*npz['psd'])**-0.5)[:4097]\n"
     ]
    }
   ],
   "source": [
    "# get data from datadir\n",
    "distributions, glitches, ifos, ml_models = get_data(datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing x train data (complex)\n",
      "x_mean_real: -0.013919275254011154, x_std_real: 91.2485580444336, x_mean_imag: -0.011076079681515694, x_std_imag: 91.03489685058594\n",
      "Normalizing y train data (y1)\n",
      "x_mean: 51.81586938742253, x_std: 77.90163836109014\n",
      "Normalizing y train data (y2)\n",
      "x_mean: 2.298970035157978, x_std: 2.109232478249102\n",
      "Normalizing x test data (complex)\n",
      "x_mean_real: -0.011282488703727722, x_std_real: 92.85208892822266, x_mean_imag: -0.010134536772966385, x_std_imag: 91.5934066772461\n",
      "Normalizing y test data (y1)\n",
      "x_mean: 51.6906381731615, x_std: 77.99759469622174\n",
      "Normalizing y test data (y2)\n",
      "x_mean: 2.3188244941313565, x_std: 2.1218213889638973\n",
      "Train data real + imag maxs: 537.1434326171875, 413.512451171875\n",
      "Test data real + imag maxs: 449.3482971191406, 351.3298645019531\n",
      "Train data real + imag mins: -380.8133850097656, -385.46002197265625\n",
      "Test data real + imag mins: -443.2416076660156, -499.0045166015625\n"
     ]
    }
   ],
   "source": [
    "# create training and test datasets\n",
    "train_data = GlitchDataset(datadir, ifos, ml_models, glitches, distributions, NTRAIN, NTEST, device, True, True, True, 'train', 'complex', None)\n",
    "test_data = GlitchDataset(datadir, ifos, ml_models, glitches, distributions, NTRAIN, NTEST, device, True, True, True, 'test', 'complex', train_data)\n",
    "\n",
    "# # print max and mins of training and test data\n",
    "print(f\"Train data real + imag maxs: {torch.max(train_data.x_arr.real)}, {torch.max(train_data.x_arr.imag)}\")\n",
    "print(f\"Test data real + imag maxs: {torch.max(test_data.x_arr.real)}, {torch.max(test_data.x_arr.imag)}\")\n",
    "print(f\"Train data real + imag mins: {torch.min(train_data.x_arr.real)}, {torch.min(train_data.x_arr.imag)}\")\n",
    "print(f\"Test data real + imag mins: {torch.min(test_data.x_arr.real)}, {torch.min(test_data.x_arr.imag)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot some data\n",
    "\n",
    "trainitem = train_data.__getitem__(0)\n",
    "testitem = test_data.__getitem__(0)\n",
    "trainitem=trainitem[0].cpu().numpy()\n",
    "testitem=testitem[0].cpu().numpy()\n",
    "\n",
    "# Get a real snippet\n",
    "ifo = np.random.choice(ifos)\n",
    "ml_model = np.random.choice(ml_models)\n",
    "glitch_num = random.choice(glitches[ifo][ml_model])\n",
    "glitch_num = glitch_num['num']\n",
    "# create a SnippetNormed object\n",
    "snip = SnippetNormed(ifo, ml_model, glitch_num, datadir)\n",
    "inf = {}\n",
    "inf['freqs'] = np.linspace(0, 4096, 513)\n",
    "snip.set_infer(inf)\n",
    "# get the glitch data in the frequency domain\n",
    "actual_item = rfft(snip.whts)\n",
    "# scale it by the training scalings\n",
    "actual_item_real = (actual_item.real - train_data.tr_x_mean_real) / train_data.tr_x_std_real\n",
    "actual_item_imag = (actual_item.imag - train_data.tr_x_mean_imag) / train_data.tr_x_std_imag\n",
    "actual_item = actual_item_real + 1j * actual_item_imag\n",
    "actual_item = np.abs(actual_item)\n",
    "# plot them all side by side\n",
    "plt.figure()\n",
    "plt.subplot(1,3,1)\n",
    "plt.loglog(np.abs(trainitem))\n",
    "plt.title('Train')\n",
    "plt.subplot(1,3,2)\n",
    "plt.loglog(np.abs(testitem))\n",
    "plt.title('Test')\n",
    "plt.subplot(1,3,3)\n",
    "plt.loglog(actual_item)\n",
    "plt.title('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastRandomSampler(Sampler[int]):\n",
    "    def __init__(self, data_source: Sized, batch_size: int, generator=None) -> None:\n",
    "        self.data_source = data_source\n",
    "        self.data_len = len(self.data_source)\n",
    "        self.batch_size = batch_size\n",
    "        self.generator = generator or torch.Generator()\n",
    "        self.epoch_indices = []\n",
    "        self.set_epoch(0)  # Shuffle at initialization\n",
    "\n",
    "    def _shuffle_indices(self):\n",
    "        self.epoch_indices = torch.randperm(self.data_len, generator=self.generator).tolist()\n",
    "\n",
    "    def __iter__(self) -> Iterator[List[int]]:\n",
    "        # Yield batches of indices using slicing\n",
    "        return (self.epoch_indices[i:i + self.batch_size] for i in range(0, self.data_len, self.batch_size))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # Number of batches in an epoch\n",
    "        return (len(self.data_source) + self.batch_size - 1) // self.batch_size\n",
    "    \n",
    "    def set_epoch(self, epoch: int) -> None:\n",
    "        self._shuffle_indices()  # Shuffle indices for the new epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 513])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load into DataLoader\n",
    "tr_sampler = FastRandomSampler(train_data, 1024)\n",
    "te_sampler = FastRandomSampler(test_data, 1024)\n",
    "collate_fn = lambda x: tuple(x)\n",
    "train_loader = DataLoader(dataset=train_data, sampler=tr_sampler,   drop_last=False)\n",
    "test_loader = DataLoader(dataset=test_data, sampler=te_sampler,   drop_last=False)\n",
    "list(train_loader.__iter__())[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, criterion, optimizer, scheduler, train_loader, test_loader, num_epochs, model_path, writer):\n",
    "    epoch_count=0\n",
    "    # Load model if it exists\n",
    "    try:\n",
    "        # find model with highest epoch number\n",
    "        modelfn = model_path.split('/')[-1]\n",
    "        modeldir = model_path.split('/')[:-1].join('/')\n",
    "        model_files = os.listdir(modeldir)\n",
    "        model_files = [f for f in model_files if f.endswith('.pt') and f.startswith(modelfn.split('.')[0])]\n",
    "        if len(model_files) > 0:\n",
    "            model_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "            model_path = model_path + model_files[-1]\n",
    "            print(f'Loading model {model_path}')\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            print('Model loaded')\n",
    "            epoch_count = int(model_path.split('_')[-1].split('.')[0])\n",
    "    except:\n",
    "        print('Model not loaded')\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    loss_arr = []\n",
    "    val_loss_arr = []\n",
    "    while epoch_count < num_epochs:\n",
    "        model.train()\n",
    "        total_training_loss = 0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, targets = data\n",
    "            # Forward pass\n",
    "            outputs = model(inputs[0])\n",
    "            loss = criterion(outputs, targets[0])\n",
    "            total_training_loss += loss.item()\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        loss_arr.append(total_training_loss/len(train_loader))\n",
    "\n",
    "        # Test the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            for i, data in enumerate(test_loader):\n",
    "                inputs, targets = data\n",
    "                outputs = model(inputs[0])\n",
    "                val_loss = criterion(outputs, targets[0]).item()\n",
    "                total_loss += val_loss\n",
    "            val_loss_arr.append(total_loss/len(test_loader))\n",
    "        print(f\"Epoch [{epoch_count+1}/{num_epochs}], Training Loss: {loss.item():.4f}, Validation Loss: {total_loss/len(test_loader):.4f}\")\n",
    "        \n",
    "        # update the learning rate\n",
    "        scheduler.step(total_loss/len(test_loader))\n",
    "\n",
    "        train_loader.sampler.set_epoch(epoch_count)\n",
    "        epoch_count += 1\n",
    "        \n",
    "        writer.add_scalars(\"Loss\", {\"Train\": total_training_loss/len(train_loader), \"Validation\": total_loss/len(test_loader)}, epoch_count)\n",
    "        writer.add_scalar(\"Learning rate\", optimizer.param_groups[0]['lr'], epoch_count)\n",
    "        \n",
    "        # save the model every 10 epochs\n",
    "        if epoch_count % 10 == 0 and epoch_count != 0:\n",
    "            torch.save(model.state_dict(), model_path.split('.')[0] + f'_{epoch_count}.pt')\n",
    "        writer.flush()\n",
    "    return loss_arr, val_loss_arr\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Valued Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): ComplexValuedNN(\n",
      "    (conv_layers): ModuleList(\n",
      "      (0): CVConv1d(\n",
      "        (conv_r): Conv1d(1, 4, kernel_size=(1,), stride=(1,), padding=same)\n",
      "        (conv_i): Conv1d(1, 4, kernel_size=(1,), stride=(1,), padding=same)\n",
      "      )\n",
      "      (1): CVCardiod()\n",
      "      (2): CVBatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): CVConv1d(\n",
      "        (conv_r): Conv1d(4, 8, kernel_size=(3,), stride=(1,), padding=same)\n",
      "        (conv_i): Conv1d(4, 8, kernel_size=(3,), stride=(1,), padding=same)\n",
      "      )\n",
      "      (4): CVCardiod()\n",
      "      (5): CVBatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (fc_layers): ModuleList(\n",
      "      (0): CVLinear(\n",
      "        (linear_r): Linear(in_features=4104, out_features=64, bias=False)\n",
      "        (linear_i): Linear(in_features=4104, out_features=64, bias=False)\n",
      "      )\n",
      "      (1): CVCardiod()\n",
      "      (2): CVLinear(\n",
      "        (linear_r): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (linear_i): Linear(in_features=64, out_features=64, bias=False)\n",
      "      )\n",
      "      (3): CVCardiod()\n",
      "    )\n",
      "    (output_layer): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 534058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xangma/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "if train_complex:\n",
    "    if any([x in locals() for x in ['model', 'criterion', 'optimizer', 'scheduler', 'writer']]):\n",
    "        del model, criterion, optimizer, scheduler, writer\n",
    "        torch.cuda.empty_cache()\n",
    "    # Create an instance of the network\n",
    "    def get_complex_model():\n",
    "        return ComplexValuedNN(n_conv_layers=2, conv_filters=[4,8], conv_kernel_size=[1,3], n_fc_layers=2, n_fc_units=64)\n",
    "    \n",
    "    model = get_complex_model()\n",
    "    # put model on device\n",
    "    model.to(device)\n",
    "\n",
    "    model = torch.compile(model)\n",
    "    # Print the model architecture\n",
    "    print(model)\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=20, verbose=True)\n",
    "    writer = SummaryWriter(\"Complex_valued\")\n",
    "    model_path = rootdir + 'notebooks/complex_nn_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xangma/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/_inductor/lowering.py:1778: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 0.5619, Validation Loss: 0.7284\n",
      "Epoch [2/100], Training Loss: 0.4839, Validation Loss: 0.5415\n",
      "Epoch [3/100], Training Loss: 0.4199, Validation Loss: 0.4411\n",
      "Epoch [4/100], Training Loss: 0.3141, Validation Loss: 0.3707\n",
      "Epoch [5/100], Training Loss: 0.2940, Validation Loss: 0.3172\n",
      "Epoch [6/100], Training Loss: 0.2259, Validation Loss: 0.2833\n",
      "Epoch [7/100], Training Loss: 0.3268, Validation Loss: 0.2561\n",
      "Epoch [8/100], Training Loss: 0.2200, Validation Loss: 0.2393\n",
      "Epoch [9/100], Training Loss: 0.2246, Validation Loss: 0.2176\n",
      "Epoch [10/100], Training Loss: 0.1493, Validation Loss: 0.2006\n",
      "Epoch [11/100], Training Loss: 0.1675, Validation Loss: 0.1989\n",
      "Epoch [12/100], Training Loss: 0.1847, Validation Loss: 0.1824\n",
      "Epoch [13/100], Training Loss: 0.1339, Validation Loss: 0.1748\n",
      "Epoch [14/100], Training Loss: 0.1408, Validation Loss: 0.1742\n",
      "Epoch [15/100], Training Loss: 0.1815, Validation Loss: 0.1626\n",
      "Epoch [16/100], Training Loss: 0.1405, Validation Loss: 0.1576\n",
      "Epoch [17/100], Training Loss: 0.1696, Validation Loss: 0.1525\n",
      "Epoch [18/100], Training Loss: 0.1703, Validation Loss: 0.1479\n",
      "Epoch [19/100], Training Loss: 0.1320, Validation Loss: 0.1461\n",
      "Epoch [20/100], Training Loss: 0.0711, Validation Loss: 0.1398\n",
      "Epoch [21/100], Training Loss: 0.1424, Validation Loss: 0.1453\n",
      "Epoch [22/100], Training Loss: 0.1118, Validation Loss: 0.1346\n",
      "Epoch [23/100], Training Loss: 0.0812, Validation Loss: 0.1332\n",
      "Epoch [24/100], Training Loss: 0.1320, Validation Loss: 0.1330\n",
      "Epoch [25/100], Training Loss: 0.1608, Validation Loss: 0.1275\n",
      "Epoch [26/100], Training Loss: 0.0723, Validation Loss: 0.1277\n",
      "Epoch [27/100], Training Loss: 0.1085, Validation Loss: 0.1311\n",
      "Epoch [28/100], Training Loss: 0.0907, Validation Loss: 0.1247\n",
      "Epoch [29/100], Training Loss: 0.1261, Validation Loss: 0.1250\n",
      "Epoch [30/100], Training Loss: 0.0838, Validation Loss: 0.1228\n",
      "Epoch [31/100], Training Loss: 0.0771, Validation Loss: 0.1190\n",
      "Epoch [32/100], Training Loss: 0.0732, Validation Loss: 0.1203\n",
      "Epoch [33/100], Training Loss: 0.0722, Validation Loss: 0.1202\n",
      "Epoch [34/100], Training Loss: 0.0739, Validation Loss: 0.1191\n",
      "Epoch [35/100], Training Loss: 0.0734, Validation Loss: 0.1167\n",
      "Epoch [36/100], Training Loss: 0.0899, Validation Loss: 0.1216\n",
      "Epoch [37/100], Training Loss: 0.0882, Validation Loss: 0.1255\n",
      "Epoch [38/100], Training Loss: 0.0660, Validation Loss: 0.1184\n",
      "Epoch [39/100], Training Loss: 0.1609, Validation Loss: 0.1145\n",
      "Epoch [40/100], Training Loss: 0.0802, Validation Loss: 0.1191\n",
      "Epoch [41/100], Training Loss: 0.0491, Validation Loss: 0.1145\n",
      "Epoch [42/100], Training Loss: 0.0920, Validation Loss: 0.1249\n",
      "Epoch [43/100], Training Loss: 0.0575, Validation Loss: 0.1159\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mif\u001b[39;00m train_complex:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     complex_train_loss_arr,  complex_val_loss_arr \u001b[39m=\u001b[39m train(model, criterion, optimizer, scheduler, train_loader, test_loader, \u001b[39m100\u001b[39;49m, model_path, writer)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# save the loss arrays into a single file\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     np\u001b[39m.\u001b[39msavez(rootdir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnotebooks/complex_loss_arrays.npz\u001b[39m\u001b[39m'\u001b[39m, train\u001b[39m=\u001b[39mcomplex_train_loss_arr, val\u001b[39m=\u001b[39mcomplex_val_loss_arr)\n",
      "\u001b[1;32m/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m inputs, targets \u001b[39m=\u001b[39m data\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targets[\u001b[39m0\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m total_training_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m prior \u001b[39m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    450\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    452\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/OneDrive/repos/antiglitch/notebooks/cvnn_models.py:54\u001b[0m, in \u001b[0;36mComplexValuedNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_layers:\n\u001b[0;32m---> 54\u001b[0m     x \u001b[39m=\u001b[39m layer(x)\n\u001b[1;32m     56\u001b[0m \u001b[39m# Transform complex valued output for the final real-valued layer\u001b[39;00m\n\u001b[1;32m     57\u001b[0m real_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((x\u001b[39m.\u001b[39mreal, x\u001b[39m.\u001b[39mimag), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# Merge real and imag parts\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/OneDrive/repos/complextorch/complextorch/nn/modules/activation/fully_complex.py:137\u001b[0m, in \u001b[0;36mCVCardiod.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: CVTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m CVTensor:\n\u001b[1;32m    129\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Computes the complex-valued cardioid activation function.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m        CVTensor: :math:`\\frac{1}{2} (1 + \\text{cos}(\\angle\\mathbf{z})) \\odot \\mathbf{z}`\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0.5\u001b[39;49m \u001b[39m*\u001b[39;49m (\u001b[39m1\u001b[39;49m \u001b[39m+\u001b[39;49m torch\u001b[39m.\u001b[39;49mcos(\u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mangle())) \u001b[39m*\u001b[39m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/fx/traceback.py:67\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[39mreturn\u001b[39;00m [current_meta\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstack_trace\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[1;32m     65\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     \u001b[39m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m traceback\u001b[39m.\u001b[39mformat_list(traceback\u001b[39m.\u001b[39;49mextract_stack()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/traceback.py:228\u001b[0m, in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     f \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39m_getframe()\u001b[39m.\u001b[39mf_back\n\u001b[0;32m--> 228\u001b[0m stack \u001b[39m=\u001b[39m StackSummary\u001b[39m.\u001b[39;49mextract(walk_stack(f), limit\u001b[39m=\u001b[39;49mlimit)\n\u001b[1;32m    229\u001b[0m stack\u001b[39m.\u001b[39mreverse()\n\u001b[1;32m    230\u001b[0m \u001b[39mreturn\u001b[39;00m stack\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/traceback.py:390\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[39mfor\u001b[39;00m f, lineno \u001b[39min\u001b[39;00m frame_gen:\n\u001b[1;32m    388\u001b[0m         \u001b[39myield\u001b[39;00m f, (lineno, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 390\u001b[0m \u001b[39mreturn\u001b[39;00m klass\u001b[39m.\u001b[39;49m_extract_from_extended_frame_gen(\n\u001b[1;32m    391\u001b[0m     extended_frame_gen(), limit\u001b[39m=\u001b[39;49mlimit, lookup_lines\u001b[39m=\u001b[39;49mlookup_lines,\n\u001b[1;32m    392\u001b[0m     capture_locals\u001b[39m=\u001b[39;49mcapture_locals)\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/traceback.py:419\u001b[0m, in \u001b[0;36mStackSummary._extract_from_extended_frame_gen\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    416\u001b[0m name \u001b[39m=\u001b[39m co\u001b[39m.\u001b[39mco_name\n\u001b[1;32m    418\u001b[0m fnames\u001b[39m.\u001b[39madd(filename)\n\u001b[0;32m--> 419\u001b[0m linecache\u001b[39m.\u001b[39;49mlazycache(filename, f\u001b[39m.\u001b[39;49mf_globals)\n\u001b[1;32m    420\u001b[0m \u001b[39m# Must defer line lookups until we have called checkcache.\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[39mif\u001b[39;00m capture_locals:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "if train_complex:\n",
    "    complex_train_loss_arr,  complex_val_loss_arr = train(model, criterion, optimizer, scheduler, train_loader, test_loader, 100, model_path, writer)\n",
    "    # save the loss arrays into a single file\n",
    "    np.savez(rootdir + 'notebooks/complex_loss_arrays.npz', train=complex_train_loss_arr, val=complex_val_loss_arr)\n",
    "    del model, criterion, optimizer, scheduler, writer\n",
    "    torch.cuda.empty_cache()\n",
    "    memsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xangma/OneDrive/repos/antiglitch/notebooks\n"
     ]
    }
   ],
   "source": [
    "if test_complex:\n",
    "    # Predictions from Complex model\n",
    "    # Load model\n",
    "    model = get_complex_model()\n",
    "    model = torch.compile(model)\n",
    "    \n",
    "    modelfn = model_path.split('/')[-1]\n",
    "    modeldir = '/'.join(model_path.split('/')[:-1])\n",
    "    model_files = os.listdir(modeldir)\n",
    "    model_files = [f for f in model_files if f.endswith('.pt') and f.startswith(modelfn.split('.')[0])]\n",
    "    if len(model_files) > 0:\n",
    "        model_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "        model_path = modeldir + '/' + model_files[-1]\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [ 0.93532395 -0.61168426]\n",
      "Target: [ 0.3724822 -0.7465734]\n",
      "\n",
      "\n",
      "Prediction: [-0.447937  2.33234 ]\n",
      "Target: [-0.4032861  2.6188056]\n",
      "\n",
      "\n",
      "Prediction: [0.10394135 1.0190282 ]\n",
      "Target: [0.07039674 0.7326371 ]\n",
      "\n",
      "\n",
      "Prediction: [ 0.5270551 -0.8078507]\n",
      "Target: [ 0.9033805 -0.7431447]\n",
      "\n",
      "\n",
      "Prediction: [ 1.0887215 -0.871844 ]\n",
      "Target: [ 1.3933632 -0.7291103]\n",
      "\n",
      "\n",
      "Prediction: [-0.4051428  -0.33421353]\n",
      "Target: [-0.46513036 -0.39182633]\n",
      "\n",
      "\n",
      "Prediction: [0.07272147 1.3131896 ]\n",
      "Target: [-0.01738029  0.88171947]\n",
      "\n",
      "\n",
      "Prediction: [ 1.5099514 -0.3756353]\n",
      "Target: [ 1.8529413 -0.4212125]\n",
      "\n",
      "\n",
      "Prediction: [ 0.26971862 -0.8986243 ]\n",
      "Target: [ 0.66254276 -0.5259549 ]\n",
      "\n",
      "\n",
      "Prediction: [-0.41406307 -0.10547346]\n",
      "Target: [-0.4233048  -0.17582156]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if test_complex:\n",
    "    # Get 10 inputs\n",
    "    inputs, targets = next(iter(test_loader))\n",
    "    outputs = model(inputs[0])\n",
    "\n",
    "    if type(outputs) == torch.Tensor:\n",
    "        outputs = outputs.cpu().detach().numpy()\n",
    "    if type(targets) == torch.Tensor:\n",
    "        targets = targets[0].cpu().detach().numpy()\n",
    "    for i in range(0,10):\n",
    "        print(f\"Prediction: {outputs[i]}\")\n",
    "        print(f\"Target: {targets[i]}\")\n",
    "        print(\"\\n\")\n",
    "    del model, inputs, targets, outputs\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Valued Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): RealValuedNN(\n",
      "    (conv_layers): ModuleList(\n",
      "      (0): Conv1d(2, 8, kernel_size=(1,), stride=(1,), padding=same)\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): Conv1d(8, 16, kernel_size=(3,), stride=(1,), padding=same)\n",
      "      (4): ReLU()\n",
      "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (fc_layers): ModuleList(\n",
      "      (0): Linear(in_features=8208, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (output_layer): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters: 530138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xangma/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "if train_real:\n",
    "    # Create an instance of the network\n",
    "    def get_real_model():\n",
    "        return RealValuedNN(n_conv_layers=2, conv_filters=[8,16], conv_kernel_size=[1,3], n_fc_layers=2, n_fc_units=64)\n",
    "\n",
    "    # put model on device\n",
    "    model = get_real_model()\n",
    "    model.to(device)\n",
    "    model = torch.compile(model)\n",
    "\n",
    "    # Print the model architecture\n",
    "    print(model)\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=20, verbose=True)\n",
    "    writer = SummaryWriter(\"Real_valued\")\n",
    "    model_path = rootdir + 'notebooks/real_nn_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not loaded\n",
      "Epoch [1/100], Training Loss: 0.7043, Validation Loss: 0.6069\n",
      "Epoch [2/100], Training Loss: 0.3431, Validation Loss: 0.4344\n",
      "Epoch [3/100], Training Loss: 0.3076, Validation Loss: 0.4753\n",
      "Epoch [4/100], Training Loss: 0.3158, Validation Loss: 0.4609\n",
      "Epoch [5/100], Training Loss: 0.1955, Validation Loss: 0.3439\n",
      "Epoch [6/100], Training Loss: 0.3177, Validation Loss: 0.5231\n",
      "Epoch [7/100], Training Loss: 0.1744, Validation Loss: 0.4716\n",
      "Epoch [8/100], Training Loss: 0.1719, Validation Loss: 0.5030\n",
      "Epoch [9/100], Training Loss: 0.2996, Validation Loss: 0.5049\n",
      "Epoch [10/100], Training Loss: 0.2672, Validation Loss: 0.6016\n",
      "Epoch [11/100], Training Loss: 0.2513, Validation Loss: 0.4027\n",
      "Epoch [12/100], Training Loss: 0.1690, Validation Loss: 0.4383\n",
      "Epoch [13/100], Training Loss: 0.1333, Validation Loss: 0.8810\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb Cell 21\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Training loop\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mif\u001b[39;00m train_real:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     real_train_loss_arr,  real_val_loss_arr \u001b[39m=\u001b[39m train(model, criterion, optimizer, scheduler, train_loader, test_loader, \u001b[39m100\u001b[39;49m, model_path, writer)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# save the loss arrays into a single file\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     np\u001b[39m.\u001b[39msavez(rootdir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnotebooks/real_loss_arrays.npz\u001b[39m\u001b[39m'\u001b[39m, train\u001b[39m=\u001b[39mreal_train_loss_arr, val\u001b[39m=\u001b[39mreal_val_loss_arr)\n",
      "\u001b[1;32m/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m inputs, targets \u001b[39m=\u001b[39m data\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targets[\u001b[39m0\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m total_training_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m prior \u001b[39m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    450\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    452\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/OneDrive/repos/antiglitch/notebooks/cvnn_models.py:100\u001b[0m, in \u001b[0;36mRealValuedNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[39m# self.fc_layers.append(nn.BatchNorm1d(n_fc_units))\u001b[39;00m\n\u001b[1;32m     95\u001b[0m         \u001b[39m# self.fc_layers.append(nn.Dropout(0.25))\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \n\u001b[1;32m     97\u001b[0m     \u001b[39m# Output Layer (real-valued)\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_layer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(n_fc_units, \u001b[39m2\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    101\u001b[0m     \u001b[39m# Combine real and imaginary parts as separate channels\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     real \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mreal\n\u001b[1;32m    103\u001b[0m     imag \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mimag\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m prior \u001b[39m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    450\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    452\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:36\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:917\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.forward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m    915\u001b[0m full_args\u001b[39m.\u001b[39mextend(params_flat)\n\u001b[1;32m    916\u001b[0m full_args\u001b[39m.\u001b[39mextend(runtime_args)\n\u001b[0;32m--> 917\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_fn(full_args)\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py:89\u001b[0m, in \u001b[0;36mmake_boxed_func.<locals>.g\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mg\u001b[39m(args):\n\u001b[0;32m---> 89\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:88\u001b[0m, in \u001b[0;36mcreate_runtime_wrapper.<locals>.runtime_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     86\u001b[0m             args_[idx] \u001b[39m=\u001b[39m args_[idx]\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     87\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39m_force_original_view_tracking(\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> 88\u001b[0m         all_outs \u001b[39m=\u001b[39m call_func_at_runtime_with_args(\n\u001b[1;32m     89\u001b[0m             compiled_fn,\n\u001b[1;32m     90\u001b[0m             args_,\n\u001b[1;32m     91\u001b[0m             disable_amp\u001b[39m=\u001b[39;49mdisable_amp,\n\u001b[1;32m     92\u001b[0m         )\n\u001b[1;32m     93\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     \u001b[39m# When we have an inference graph, we run with torch.no_grad.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[39m# It's possible to get an inference graph with inputs that require grad,\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[39m# in which case we want to make sure autograd is disabled\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[39m# (since e.g., inductor will generate aten.addmm.out calls which autograd will complain on)\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_grad_enabled():\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py:113\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mwith\u001b[39;00m context():\n\u001b[1;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(f, \u001b[39m\"\u001b[39m\u001b[39m_boxed_call\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 113\u001b[0m         out \u001b[39m=\u001b[39m normalize_as_list(f(args))\n\u001b[1;32m    114\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m         \u001b[39m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    116\u001b[0m         \u001b[39m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    117\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    118\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt take boxed arguments. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py:89\u001b[0m, in \u001b[0;36mmake_boxed_func.<locals>.g\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mg\u001b[39m(args):\n\u001b[0;32m---> 89\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:505\u001b[0m, in \u001b[0;36maot_dispatch_autograd.<locals>.CompiledFunction.forward\u001b[0;34m(ctx, *deduped_flat_tensor_args)\u001b[0m\n\u001b[1;32m    498\u001b[0m         args \u001b[39m=\u001b[39m (\u001b[39m*\u001b[39margs, seed, offset)\n\u001b[1;32m    499\u001b[0m     \u001b[39m# There is a pretty complicated calling convention around what the compiled fw returns.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[39m# The full list of outputs and their relative order is:\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     \u001b[39m# (*tokens, *mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)\u001b[39;00m\n\u001b[1;32m    502\u001b[0m     \u001b[39m# - Note that in the synthetic bases case, mutated_inputs will correspond to an updated version\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39m#   of the original view, and not the synthetic base\u001b[39;00m\n\u001b[0;32m--> 505\u001b[0m     fw_outs \u001b[39m=\u001b[39m call_func_at_runtime_with_args(\n\u001b[1;32m    506\u001b[0m         CompiledFunction\u001b[39m.\u001b[39;49mcompiled_fw,\n\u001b[1;32m    507\u001b[0m         args,\n\u001b[1;32m    508\u001b[0m         disable_amp\u001b[39m=\u001b[39;49mdisable_amp,\n\u001b[1;32m    509\u001b[0m     )\n\u001b[1;32m    510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    511\u001b[0m     \u001b[39mnonlocal\u001b[39;00m fakified_out\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py:111\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mlist\u001b[39m)\n\u001b[1;32m    110\u001b[0m context \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_DisableAutocast \u001b[39mif\u001b[39;00m disable_amp \u001b[39melse\u001b[39;00m nullcontext\n\u001b[0;32m--> 111\u001b[0m \u001b[39mwith\u001b[39;49;00m context():\n\u001b[1;32m    112\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mhasattr\u001b[39;49m(f, \u001b[39m\"\u001b[39;49m\u001b[39m_boxed_call\u001b[39;49m\u001b[39m\"\u001b[39;49m):\n\u001b[1;32m    113\u001b[0m         out \u001b[39m=\u001b[39;49m normalize_as_list(f(args))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "if train_real:\n",
    "    real_train_loss_arr,  real_val_loss_arr = train(model, criterion, optimizer, scheduler, train_loader, test_loader, 100, model_path, writer)\n",
    "    # save the loss arrays into a single file\n",
    "    np.savez(rootdir + 'notebooks/real_loss_arrays.npz', train=real_train_loss_arr, val=real_val_loss_arr)\n",
    "    del model, criterion, optimizer, scheduler, writer\n",
    "    torch.cuda.empty_cache()\n",
    "    memsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_real:\n",
    "    # Predictions from Real model\n",
    "    # Load model\n",
    "    model = get_real_model()\n",
    "    model = torch.compile(model)\n",
    "    \n",
    "    modelfn = model_path.split('/')[-1]\n",
    "    modeldir = '/'.join(model_path.split('/')[:-1])\n",
    "    model_files = os.listdir(modeldir)\n",
    "    model_files = [f for f in model_files if f.endswith('.pt') and f.startswith(modelfn.split('.')[0])]\n",
    "    if len(model_files) > 0:\n",
    "        model_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "        model_path = modeldir + '/' + model_files[-1]\n",
    "        \n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [ 0.786638   -0.97522587]\n",
      "Target: [ 0.3724822 -0.7465734]\n",
      "\n",
      "\n",
      "Prediction: [-0.38962966  3.0105054 ]\n",
      "Target: [-0.4032861  2.6188056]\n",
      "\n",
      "\n",
      "Prediction: [-0.03902785  0.5717716 ]\n",
      "Target: [0.07039674 0.7326371 ]\n",
      "\n",
      "\n",
      "Prediction: [ 0.7250926 -0.8734434]\n",
      "Target: [ 0.9033805 -0.7431447]\n",
      "\n",
      "\n",
      "Prediction: [ 1.2181728 -0.8435024]\n",
      "Target: [ 1.3933632 -0.7291103]\n",
      "\n",
      "\n",
      "Prediction: [-0.5073728 -0.3686132]\n",
      "Target: [-0.46513036 -0.39182633]\n",
      "\n",
      "\n",
      "Prediction: [-0.07981759  0.8482362 ]\n",
      "Target: [-0.01738029  0.88171947]\n",
      "\n",
      "\n",
      "Prediction: [ 1.7655379 -0.3097226]\n",
      "Target: [ 1.8529413 -0.4212125]\n",
      "\n",
      "\n",
      "Prediction: [ 0.60366744 -0.27418873]\n",
      "Target: [ 0.66254276 -0.5259549 ]\n",
      "\n",
      "\n",
      "Prediction: [-0.4704152  -0.21185108]\n",
      "Target: [-0.4233048  -0.17582156]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if test_real:\n",
    "    # Get 10 inputs\n",
    "    inputs, targets = next(iter(test_loader))\n",
    "    outputs = model(inputs[0])\n",
    "\n",
    "    if type(outputs) == torch.Tensor:\n",
    "        outputs = outputs.cpu().detach().numpy()\n",
    "    if type(targets) == torch.Tensor:\n",
    "        targets = targets[0].cpu().detach().numpy()\n",
    "    for i in range(0,10):\n",
    "        print(f\"Prediction: {outputs[i]}\")\n",
    "        print(f\"Target: {targets[i]}\")\n",
    "        print(\"\\n\")\n",
    "    del model, inputs, targets, outputs\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load any existing complex and real loss arrays\n",
    "if 'complex_train_loss_arr' not in locals():\n",
    "    complex_loss_arrays = np.load(rootdir + 'notebooks/complex_loss_arrays.npz')\n",
    "    complex_train_loss_arr = complex_loss_arrays['train']\n",
    "    complex_val_loss_arr = complex_loss_arrays['val']\n",
    "if 'real_train_loss_arr' not in locals():\n",
    "    real_loss_arrays = np.load(rootdir + 'notebooks/real_loss_arrays.npz')\n",
    "    real_train_loss_arr = real_loss_arrays['train']\n",
    "    real_val_loss_arr = real_loss_arrays['val']\n",
    "\n",
    "# Plot the loss arrays\n",
    "plt.figure()\n",
    "plt.plot(complex_train_loss_arr, label='Complex Train')\n",
    "plt.plot(complex_val_loss_arr, label='Complex Validation')\n",
    "plt.plot(real_train_loss_arr, label='Real Train')\n",
    "plt.plot(real_val_loss_arr, label='Real Validation')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Valued in the time domain ( DEV/UNTESTED CODE BELOW )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_data, test_data, train_loader, test_loader\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "train_data = GlitchDataset(datadir, ifos, ml_models, glitches, distributions, NTRAIN, NTEST, device, True, 'train', 'real')\n",
    "test_data = GlitchDataset(datadir, ifos, ml_models, glitches, distributions, NTRAIN, NTEST, device, True, 'test', 'real')\n",
    "tr_sampler = FastRandomSampler(train_data, 1024)\n",
    "te_sampler = FastRandomSampler(test_data, 1024)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data,sampler = tr_sampler,  drop_last=False)\n",
    "test_loader = DataLoader(dataset=test_data, sampler = te_sampler, drop_last=False)\n",
    "# testitem = test_data.__getitem__(0)\n",
    "# print(testitem[0][1:20], testitem[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb Cell 28\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mantiglitch\u001b[39;00m \u001b[39mimport\u001b[39;00m SnippetNormed\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m trainitem \u001b[39m=\u001b[39m train_data\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mrandint(\u001b[39m0\u001b[39;49m, NTRAIN))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m testitem \u001b[39m=\u001b[39m test_data\u001b[39m.\u001b[39m\u001b[39m__getitem__\u001b[39m(np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, NTEST))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Get a real snippet\u001b[39;00m\n",
      "File \u001b[0;32m~/OneDrive/repos/antiglitch/notebooks/cvnn_data.py:249\u001b[0m, in \u001b[0;36mGlitchDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m--> 249\u001b[0m \t\u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_arr[idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_arr[idx]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from antiglitch import SnippetNormed\n",
    "trainitem = train_data.__getitem__(np.random.randint(0, NTRAIN))\n",
    "testitem = test_data.__getitem__(np.random.randint(0, NTEST))\n",
    "\n",
    "# Get a real snippet\n",
    "ifo = np.random.choice(ifos)\n",
    "ml_model = np.random.choice(ml_models)\n",
    "glitch_num = random.choice(glitches[ifo][ml_model])\n",
    "glitch_num = glitch_num['num']\n",
    "# create a SnippetNormed object\n",
    "snip = SnippetNormed(ifo, ml_model, glitch_num, datadir)\n",
    "inf = {}\n",
    "inf['freqs'] = np.linspace(0, 4096, 513)\n",
    "snip.set_infer(inf)\n",
    "# get the glitch data in the time domain\n",
    "actual_item = snip.whts\n",
    "# scale it by the training scalings\n",
    "actual_item = (actual_item - train_data.x_arr_mean) / train_data.x_arr_std\n",
    "# plot them all side by side\n",
    "plt.figure()\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(trainitem[0].cpu().numpy())\n",
    "plt.title('Train')\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(testitem[0].cpu().numpy())\n",
    "plt.title('Test')\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(actual_item)\n",
    "plt.title('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real Valued Training\n",
    "input_dim = 513  # Number of input features\n",
    "output_dim = 2  # Number of output units (as specified in the dataset)\n",
    "# reimport the real valued model from cvnn_models\n",
    "from cvnn_models import RealValuedNNtd\n",
    "\n",
    "# Create an instance of the network\n",
    "model = RealValuedNNtd()\n",
    "\n",
    "# put model on device\n",
    "model.to(device)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=20, verbose=True)\n",
    "writer = SummaryWriter(\"Real_valued_td\")\n",
    "model_path = rootdir + 'notebooks/real_td_nn_model.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "if train_real_td:\n",
    "    train(model, criterion, optimizer, scheduler, train_loader, test_loader, 100, model_path, writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1024*16384)-(65536*256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del model, criterion, optimizer, scheduler, writer\n",
    "torch.cuda.empty_cache()\n",
    "memsum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if test_real_td:\n",
    "    # Predictions from Real model\n",
    "    # Load model\n",
    "    model = RealValuedNN()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if test_real_td:\n",
    "    # Get 10 inputs\n",
    "    inputs, targets = next(iter(test_loader))\n",
    "    outputs = model(inputs[0].real, inputs[0].imag)\n",
    "\n",
    "    if type(outputs) == torch.Tensor:\n",
    "        outputs = outputs.cpu().detach().numpy()\n",
    "    if type(targets) == torch.Tensor:\n",
    "        targets = targets[0].cpu().detach().numpy()\n",
    "    for i in range(0,10):\n",
    "        print(f\"Prediction: {outputs[i]}\")\n",
    "        print(f\"Target: {targets[i]}\")\n",
    "        print(\"\\n\")\n",
    "del model, inputs, targets, outputs\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "antiglitch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
