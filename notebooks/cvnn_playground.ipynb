{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import complextorch as cvtorch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import re\n",
    "\n",
    "import antiglitch\n",
    "from antiglitch import to_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_complex = True\n",
    "train_real = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = '/home/xangma/OneDrive/repos/antiglitch/'\n",
    "datadir = rootdir + 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# files are named as follows: ifo-key-num.npz,\n",
    "# e.g. L1-lowblip-0355.npz\n",
    "\n",
    "# Find all ifos, keys, and numbers in the data directory\n",
    "ifos = []\n",
    "ml_models = []\n",
    "numbers = []\n",
    "\n",
    "# filename pattern: ifo-key-num.npz\n",
    "filename_pattern = re.compile(r'([A-Z0-9]+)-([a-z]+)-(\\d+).npz')\n",
    "\n",
    "for file in os.listdir(datadir):\n",
    "\t# Get unique ifos:\n",
    "\ttry:\n",
    "\t\t# check filename format\n",
    "\t\tif filename_pattern.match(file):\n",
    "\t\t\tifo = file.split('-')[0]\n",
    "\t\t\tif ifo not in ifos:\n",
    "\t\t\t\tifos.append(ifo)\n",
    "\t\t\t# Get unique keys:\n",
    "\t\t\tml_model = file.split('-')[1]\n",
    "\t\t\tif ml_model not in ml_models:\n",
    "\t\t\t\tml_models.append(ml_model)\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\n",
    "datadict={}\n",
    "for ifo in ifos:\n",
    "\tdatadict[ifo] = {}\n",
    "\tfor ml_model in ml_models:\n",
    "\t\tdatadict[ifo][ml_model] = []\n",
    "\t\tfor file in os.listdir(datadir):\n",
    "\t\t\ttry:\n",
    "\t\t\t\tifo_file = file.split('-')[0]\n",
    "\t\t\t\tkey_file = file.split('-')[1]\n",
    "\t\t\t\tnum = file.split('-')[2].split('.')[0]\n",
    "\t\t\t\tif ifo_file == ifo and key_file == ml_model:\n",
    "\t\t\t\t\tdatadict[ifo][ml_model].append(int(num))\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results\n",
    "resultsjson=datadir + 'all_PE_v3.json'\n",
    "with open(resultsjson, 'r') as f:\n",
    "\tresults = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all glitches into an array using the datadict and Snippet class\n",
    "glitches = {}\n",
    "for ifo in ifos:\n",
    "    if ifo not in glitches:\n",
    "        glitches[ifo] = {}\n",
    "    for ml_model in ml_models:\n",
    "        if ml_model not in glitches[ifo]:\n",
    "            glitches[ifo][ml_model] = {}\n",
    "        for num in datadict[ifo][ml_model]:\n",
    "            # try:\n",
    "                if num not in glitches[ifo][ml_model]:\n",
    "                    glitches[ifo][ml_model][num] = {}\n",
    "                snip = antiglitch.SnippetNormed(ifo, ml_model, num, datadir)\n",
    "                glitches[ifo][ml_model][num]['data'] = to_fd(snip.whts)\n",
    "                glitches[ifo][ml_model][num]['invasd'] = snip.invasd\n",
    "            # except:\n",
    "            #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_label_map = {'Blip_Low_Frequency':'lowblip', 'Blip':'blip', 'Koi_Fish':'koi', 'Tomte':'tomte'}\n",
    "results_keys = ['f0', 'f0_sd', 'gbw', 'gbw_sd', 'amp_r', 'amp_r_sd', 'amp_i', 'amp_i_sd', 'time', 'time_sd', 'num']\n",
    "\n",
    "nums = list(results['num'].values())\n",
    "for i in range(len(nums)):\n",
    "    try:\n",
    "        stri = str(i)\n",
    "        ifo = results['ifo'][stri]\n",
    "        ml_label = ml_label_map[results['ml_label'][stri]]\n",
    "        \n",
    "        for key in results_keys:\n",
    "            glitches[ifo][ml_label][nums[i]][key] = results[key][stri]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Restructure the dataset (Yes I know it was originally in this format, you leave me alone)\n",
    "distributions = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "\n",
    "for ifo in glitches:\n",
    "    for ml_model in glitches[ifo]:\n",
    "        for glitch_num in glitches[ifo][ml_model]:\n",
    "            for key in results_keys:\n",
    "                distributions[ifo][ml_model][key].append(glitches[ifo][ml_model][glitch_num][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sample(ifo, ml_model, tosample=['f0', 'gbw', 'amp_r', 'amp_i', 'time']):\n",
    "    res = {key:None for key in tosample}\n",
    "    for key in tosample:\n",
    "        draw = np.random.choice(distributions[ifo][ml_model][key])\n",
    "        draw_sd = distributions[ifo][ml_model][key + '_sd'][distributions[ifo][ml_model][key].index(draw)]\n",
    "        draw_final = np.random.normal(draw, draw_sd)\n",
    "        res[key] = draw_final\n",
    "    return res\n",
    "\n",
    "def new_init_Snippet(self, invasd):\n",
    "        self.invasd = invasd\n",
    "\n",
    "SnippetNormed = type('SnippetNormed', (antiglitch.SnippetNormed,), {'__init__': new_init_Snippet})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snip(ifos, ml_models, tosample=['f0', 'gbw', 'amp_r', 'amp_i', 'time']):\n",
    "\t\tifo = np.random.choice(ifos)\n",
    "\t\tml_model = np.random.choice(ml_models)\n",
    "\t\tglitch_num = random.choice(datadict[ifo][ml_model])\n",
    "\t\tglitch_invasd = glitches[ifo][ml_model][glitch_num]['invasd']\n",
    "\t\tsnip = SnippetNormed(glitch_invasd)\n",
    "\t\tinf = gen_sample(ifo, ml_model)\n",
    "\t\tinf['freqs'] = snip.invasd\n",
    "\t\tsnip.set_infer(inf)\n",
    "\t\tx = snip.fglitch\n",
    "\t\t# check for nans\n",
    "\t\tif np.isnan(x).any():\n",
    "\t\t\treturn get_snip(ifos, ml_models, tosample)\n",
    "\t\ty = (snip.inf['f0'], snip.inf['gbw'])\n",
    "\t\treturn x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0038+0.0035j, -0.0185+0.0791j, -0.1046+0.3551j, -0.2490+0.7901j,\n",
      "        -0.3985+1.2057j, -0.4960+1.4394j, -0.5424+1.5138j, -0.6074+1.6317j,\n",
      "        -0.6671+1.7271j, -0.6741+1.6844j, -0.6812+1.6439j, -0.7071+1.6490j,\n",
      "        -0.7051+1.5906j, -0.6782+1.4812j, -0.6639+1.4043j, -0.6509+1.3341j,\n",
      "        -0.6241+1.2404j, -0.6020+1.1607j, -0.5896+1.1031j], device='cuda:0') tensor([-0.2473, -0.7574], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "NTRAIN = 200000\n",
    "NTEST = 50000\n",
    "cachedataset_prefix = \"antiglitch_cvnn_dataset_\"\n",
    "\n",
    "class GlitchDataset(torch.utils.data.TensorDataset):\n",
    "\tdef __init__(self, ifos, ml_models, datadict, glitches, distributions, size, tr_scalings=None):\n",
    "\t\tself.ifos = ifos\n",
    "\t\tself.ml_models = ml_models\n",
    "\t\tself.datadict = datadict\n",
    "\t\tself.glitches = glitches\n",
    "\t\tself.distributions = distributions\n",
    "\t\tself.size = size\n",
    "\t\tself.x_arr = []\n",
    "\t\tself.y_arr = []\n",
    "\t\tif tr_scalings != None:\n",
    "\t\t\tself.x_mean_real, self.x_std_real, self.x_mean_imag, self.x_std_imag, self.y1_mean, self.y1_std, self.y2_mean, self.y2_std = tr_scalings\n",
    "\t\t# create dataset\n",
    "\t\tif cachedataset_prefix + str(self.size) + '.npz' in os.listdir(datadir):\n",
    "\t\t\tdata = np.load(datadir + cachedataset_prefix + str(self.size) + '.npz')\n",
    "\t\t\tself.x_arr = data['x_arr']\n",
    "\t\t\tself.y_arr = data['y_arr']\n",
    "\t\t\tif tr_scalings == None:\n",
    "\t\t\t\tself.x_mean_real = np.mean(self.x_arr.real)\n",
    "\t\t\t\tself.x_std_real = np.std(self.x_arr.real)\n",
    "\t\t\t\t\n",
    "\t\t\t\tself.x_mean_imag = np.mean(self.x_arr.imag)\n",
    "\t\t\t\tself.x_std_imag = np.std(self.x_arr.imag)\n",
    "\t\t\t\tself.y1_mean = self.y_arr[:,0].mean()\n",
    "\t\t\t\tself.y1_std = self.y_arr[:,0].std()\n",
    "\t\t\t\tself.y2_mean = self.y_arr[:,1].mean()\n",
    "\t\t\t\tself.y2_std = self.y_arr[:,1].std()\n",
    "\t\telse:\n",
    "\t\t\tfor i in range(size):\n",
    "\t\t\t\tx, y = get_snip(ifos, ml_models)\n",
    "\t\t\t\tself.x_arr.append(x)\n",
    "\t\t\t\tself.y_arr.append(y)\n",
    "\t\t\tself.x_arr = np.array(self.x_arr)\n",
    "\t\t\tself.y_arr = np.array(self.y_arr)\n",
    "\t\t\t# normalize x_arr\n",
    "\t\t\treal_parts = np.real(self.x_arr)\n",
    "\t\t\timag_parts = np.imag(self.x_arr)\n",
    "\t\t\t\n",
    "\t\t\tif tr_scalings == None:\n",
    "\t\t\t\tself.x_mean_real = np.mean(real_parts)\n",
    "\t\t\t\tself.x_std_real = np.std(real_parts)\n",
    "\t\t\t\t\n",
    "\t\t\t\tself.x_mean_imag = np.mean(imag_parts)\n",
    "\t\t\t\tself.x_std_imag = np.std(imag_parts)\n",
    "\t\t\t\n",
    "\t\t\tnormalized_real = (real_parts - self.x_mean_real) / self.x_std_real\n",
    "\t\t\tnormalized_imag = (imag_parts - self.x_mean_imag) / self.x_std_imag\n",
    "\t\t\t\n",
    "\t\t\tself.x_arr = normalized_real + 1j * normalized_imag\n",
    "\t\t\t# normalize y_arr\n",
    "\t\t\tif tr_scalings == None:\n",
    "\t\t\t\tself.y1_mean = self.y_arr[:,0].mean()\n",
    "\t\t\t\tself.y1_std = self.y_arr[:,0].std()\n",
    "\t\t\t\tself.y2_mean = self.y_arr[:,1].mean()\n",
    "\t\t\t\tself.y2_std = self.y_arr[:,1].std()\n",
    "\t\t\tself.y_arr[:,0] = (self.y_arr[:,0] - self.y1_mean)/self.y1_std\n",
    "\t\t\tself.y_arr[:,1] = (self.y_arr[:,1] - self.y2_mean)/self.y2_std\n",
    "\t\t\tnp.savez(datadir + cachedataset_prefix + str(size) + '.npz', x_arr=self.x_arr, y_arr=self.y_arr)\n",
    "\t\t# convert to tensors\n",
    "\t\tself.x_arr = torch.tensor(self.x_arr, dtype=torch.complex64, device=device)\n",
    "\t\tself.y_arr = torch.tensor(self.y_arr, dtype=torch.float32, device=device)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.size\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn self.x_arr[idx], self.y_arr[idx]\n",
    "\n",
    "train_data = GlitchDataset(ifos, ml_models, datadict, glitches, distributions, NTRAIN)\n",
    "tr_scalings = [train_data.x_mean_real, train_data.x_std_real, train_data.x_mean_imag, train_data.x_std_imag, train_data.y1_mean, train_data.y1_std, train_data.y2_mean, train_data.y2_std]\n",
    "test_data = GlitchDataset(ifos, ml_models, datadict, glitches, distributions, NTEST, tr_scalings=tr_scalings)\n",
    "testitem = test_data.__getitem__(0)\n",
    "print(testitem[0][1:20], testitem[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These were cells used for preprocessing the data before I was told I could use the Snippet class to generate - whoops.\n",
    "# # Now flatten to x and y data where x is glitched data and y is the [f0, gbw]\n",
    "\n",
    "# x = []\n",
    "# y = []\n",
    "# for ifo in ifos:\n",
    "# \tfor key in keys:\n",
    "# \t\tfor num in datadict[ifo][key]:\n",
    "# \t\t\ttry:\n",
    "# \t\t\t\tx.append(glitches[ifo][key][num]['data'])\n",
    "# \t\t\t\ty.append([glitches[ifo][key][num]['f0'], glitches[ifo][key][num]['gbw']])\n",
    "# \t\t\texcept:\n",
    "# \t\t\t\tpass\n",
    "# x = np.array(x)\n",
    "# y = np.array(y)\n",
    "# type(x), x.shape, type(y), y.shape\n",
    "# # Split data into training and testing\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Convert to torch tensors\n",
    "# x_train = torch.tensor(x_train, dtype=torch.complex64)\n",
    "# x_test = torch.tensor(x_test, dtype=torch.complex64)\n",
    "# y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "# y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "# # Normalize data\n",
    "# x_train = (x_train - x_train.mean()) / x_train.std()\n",
    "# x_test = (x_test - x_test.mean()) / x_test.std()\n",
    "# y_train = (y_train - y_train.mean()) / y_train.std()\n",
    "# y_test = (y_test - y_test.mean()) / y_test.std()\n",
    "# # put data on device\n",
    "# x_train = x_train.to(device)\n",
    "# x_test = x_test.to(device)\n",
    "# y_train = y_train.to(device)\n",
    "# y_test = y_test.to(device)\n",
    "# # Print out a description and sample of the data\n",
    "# print(x_train.shape, x_train.dtype, x_train[0])\n",
    "# print(x_test.shape, x_test.dtype, x_test[0])\n",
    "# print(y_train.shape, y_train.dtype, y_train[0])\n",
    "# print(y_test.shape, y_test.dtype, y_test[0])\n",
    "# train_data = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "# test_data = torch.utils.data.TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 513])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load into DataLoader\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=1024, shuffle=True, drop_last=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1024, shuffle=False, drop_last=False)\n",
    "list(train_loader.__iter__())[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexValuedNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexValuedNN, self).__init__()\n",
    "\n",
    "        # Complex Conv Layer 1\n",
    "        self.conv1 = cvtorch.nn.CVConv1d(1, 32, 3, padding=\"same\")\n",
    "        self.activation1 = cvtorch.nn.CVCardiod()\n",
    "        # self.bn1 = cvtorch.nn.CVBatchNorm1d(32)\n",
    "        # self.dropout1 = cvtorch.nn.CVDropout(0.25)\n",
    "\n",
    "        # Complex Conv Layer 2\n",
    "        self.conv2 = cvtorch.nn.CVConv1d(32, 64, 5, padding=\"same\")\n",
    "        self.activation2 = cvtorch.nn.CVCardiod()\n",
    "        # self.bn2 = cvtorch.nn.CVBatchNorm1d(64)\n",
    "        # self.dropout2 = cvtorch.nn.CVDropout(0.25)\n",
    "\n",
    "        # Fully connected Layer 1\n",
    "        self.fc1 = cvtorch.nn.CVLinear(64 * 513, 128)\n",
    "        self.activation3 = cvtorch.nn.CVCardiod()\n",
    "        # self.bn3 = cvtorch.nn.CVBatchNorm1d(1024)\n",
    "        # self.dropout3 = cvtorch.nn.CVDropout(0.25)\n",
    "\n",
    "        # Fully connected Layer 2\n",
    "        # self.fc2 = cvtorch.nn.CVLinear(1024, 1024)\n",
    "        # self.activation4 = cvtorch.nn.zReLU()\n",
    "        # self.bn4 = cvtorch.nn.CVBatchNorm1d(1024)\n",
    "        # self.dropout4 = cvtorch.nn.CVDropout(0.25)\n",
    "        # Fully connected Layer 2\n",
    "        self.fc3 = cvtorch.nn.CVLinear(128, 32)\n",
    "        self.activation5 = cvtorch.nn.CVCardiod()\n",
    "        \n",
    "        # self.bn5 = cvtorch.nn.CVBatchNorm1d(32)\n",
    "        # self.dropout5 = cvtorch.nn.CVDropout(0.25)\n",
    "        \n",
    "        # Output Layer (real-valued)\n",
    "        self.output_layer = nn.Linear(32 * 2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add channel dimension for convolution\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        # Conv Layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation1(x)\n",
    "        # x = self.bn1(x)\n",
    "        # x = self.dropout1(x)\n",
    "\n",
    "        # # # Conv Layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation2(x)\n",
    "        # x = self.bn2(x)\n",
    "        # x = self.dropout2(x)\n",
    "\n",
    "        # Flatten the tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully Connected Layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation3(x)\n",
    "        # x = self.bn3(x)\n",
    "        # x = self.dropout3(x)\n",
    "\n",
    "        # Fully Connected Layer 2\n",
    "        # x = self.fc2(x)\n",
    "        # x = self.activation4(x)\n",
    "        # x = self.bn4(x)\n",
    "        # x = self.dropout4(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.activation5(x)\n",
    "        # x = self.bn5(x)\n",
    "        # x = self.dropout5(x)\n",
    "        # Transform complex valued output for the final real-valued layer\n",
    "        real_x = torch.cat((x.real, x.imag), dim=1)  # Merge real and imag parts\n",
    "\n",
    "        # Output Layer\n",
    "        out = self.output_layer(real_x)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "class RealValuedNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RealValuedNN, self).__init__()\n",
    "\n",
    "        # Conv Layer 1\n",
    "        self.conv1_real = nn.Conv1d(1, 32, kernel_size=3, padding='same')\n",
    "        self.conv1_imag = nn.Conv1d(1, 32, kernel_size=3, padding='same')\n",
    "\n",
    "        # Activation 1\n",
    "        self.activation1_real = nn.ReLU()\n",
    "        self.activation1_imag = nn.ReLU()\n",
    "\n",
    "        # Conv Layer 2\n",
    "        self.conv2_real = nn.Conv1d(32, 64, kernel_size=5, padding='same')\n",
    "        self.conv2_imag = nn.Conv1d(32, 64, kernel_size=5, padding='same')\n",
    "\n",
    "        # Activation 2\n",
    "        self.activation2_real = nn.ReLU()\n",
    "        self.activation2_imag = nn.ReLU()\n",
    "\n",
    "        # Given input size is (512), Convolution does not alter the dimension with padding='same'\n",
    "        # Therefore, the size is the same after convolutions (512).\n",
    "        # Flatten the tensor\n",
    "        self.flatten_size = 64 * 513  # Adjusted size to reflect conv2 output * sequence length\n",
    "\n",
    "        # Fully connected Layer 1\n",
    "        self.fc1_real = nn.Linear(self.flatten_size, 128)\n",
    "        self.fc1_imag = nn.Linear(self.flatten_size, 128)\n",
    "\n",
    "        # Activation 3\n",
    "        self.activation3_real = nn.ReLU()\n",
    "        self.activation3_imag = nn.ReLU()\n",
    "\n",
    "        # Fully connected Layer 2\n",
    "        self.fc2_real = nn.Linear(128, 32)\n",
    "        self.fc2_imag = nn.Linear(128, 32)\n",
    "\n",
    "        # Activation 4\n",
    "        self.activation4_real = nn.ReLU()\n",
    "        self.activation4_imag = nn.ReLU()\n",
    "        \n",
    "        # Output Layer (real-valued)\n",
    "        self.output_layer = nn.Linear(32 * 2, 2)\n",
    "\n",
    "    def forward(self, real, imag):\n",
    "        # Add channel dimension for convolution\n",
    "        real = real.unsqueeze(1)\n",
    "        imag = imag.unsqueeze(1)\n",
    "\n",
    "        # Conv Layer 1\n",
    "        real = self.conv1_real(real)\n",
    "        imag = self.conv1_imag(imag)\n",
    "        real = self.activation1_real(real)\n",
    "        imag = self.activation1_imag(imag)\n",
    "\n",
    "        # Conv Layer 2\n",
    "        real = self.conv2_real(real)\n",
    "        imag = self.conv2_imag(imag)\n",
    "        real = self.activation2_real(real)\n",
    "        imag = self.activation2_imag(imag)\n",
    "\n",
    "        # Flatten the tensors\n",
    "        real = real.view(real.size(0), -1)\n",
    "        imag = imag.view(imag.size(0), -1)\n",
    "        \n",
    "        # Fully Connected Layer 1\n",
    "        real = self.fc1_real(real)\n",
    "        imag = self.fc1_imag(imag)\n",
    "        real = self.activation3_real(real)\n",
    "        imag = self.activation3_imag(imag)\n",
    "\n",
    "        # Fully Connected Layer 2\n",
    "        real = self.fc2_real(real)\n",
    "        imag = self.fc2_imag(imag)\n",
    "        real = self.activation4_real(real)\n",
    "        imag = self.activation4_imag(imag)\n",
    "\n",
    "        # Concatenate the real and imaginary parts\n",
    "        real_imag = torch.cat((real, imag), dim=1)\n",
    "\n",
    "        # Output Layer\n",
    "        out = self.output_layer(real_imag)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Test the model with dummy input\n",
    "model = RealValuedNN()\n",
    "model.eval()\n",
    "real_input = torch.randn(32, 513)  # Batch size of 32 and feature size of 1024 for real part\n",
    "imag_input = torch.randn(32, 513)  # Batch size of 32 and feature size of 1024 for imaginary part\n",
    "output = model(real_input, imag_input)\n",
    "print(output.shape) # Expecting (32, 2)\n",
    "del model, real_input, imag_input, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, criterion, optimizer, scheduler, train_loader, test_loader, num_epochs, model_path, writer):\n",
    "    # Load model if it exists\n",
    "    try:\n",
    "        if os.path.exists(model_path):\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            print('Model loaded')\n",
    "    except:\n",
    "        print('Model not loaded')\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            # Forward pass\n",
    "            if isinstance(model, ComplexValuedNN):\n",
    "                outputs = model(inputs)\n",
    "            elif isinstance(model, RealValuedNN):\n",
    "                outputs = model(torch.real(inputs), torch.imag(inputs))\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Test the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            for inputs, targets in test_loader:\n",
    "                if isinstance(model, ComplexValuedNN):\n",
    "                    outputs = model(inputs)\n",
    "                elif isinstance(model, RealValuedNN):\n",
    "                    outputs = model(torch.real(inputs), torch.imag(inputs))\n",
    "                total_loss += criterion(outputs, targets).item()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}, Validation Loss: {total_loss/len(test_loader):.4f}\")\n",
    "        \n",
    "        # update the learning rate\n",
    "        scheduler.step(total_loss/len(test_loader))\n",
    "\n",
    "        writer.add_scalars(\"Loss\", {\"Train\": loss.item(), \"Validation\": total_loss/len(test_loader)}, epoch)\n",
    "        writer.add_scalar(\"Learning rate\", optimizer.param_groups[0]['lr'], epoch)\n",
    "        \n",
    "        # save the model every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "    writer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Valued Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComplexValuedNN(\n",
      "  (conv1): CVConv1d(\n",
      "    (conv_r): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (conv_i): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  )\n",
      "  (activation1): CVCardiod()\n",
      "  (conv2): CVConv1d(\n",
      "    (conv_r): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=same)\n",
      "    (conv_i): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=same)\n",
      "  )\n",
      "  (activation2): CVCardiod()\n",
      "  (fc1): CVLinear(\n",
      "    (linear_r): Linear(in_features=32832, out_features=128, bias=False)\n",
      "    (linear_i): Linear(in_features=32832, out_features=128, bias=False)\n",
      "  )\n",
      "  (activation3): CVCardiod()\n",
      "  (fc3): CVLinear(\n",
      "    (linear_r): Linear(in_features=128, out_features=32, bias=False)\n",
      "    (linear_i): Linear(in_features=128, out_features=32, bias=False)\n",
      "  )\n",
      "  (activation5): CVCardiod()\n",
      "  (output_layer): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "Number of parameters: 8434178\n"
     ]
    }
   ],
   "source": [
    "# Define the input and output dimensions based on the dataset\n",
    "input_dim = 513  # Number of input features\n",
    "output_dim = 2  # Number of output units (as specified in the dataset)\n",
    "\n",
    "# Create an instance of the network\n",
    "model = ComplexValuedNN()\n",
    "\n",
    "# put model on device\n",
    "model.to(device)\n",
    "\n",
    "# model = torch.compile(model)\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=20, verbose=True)\n",
    "writer = SummaryWriter(\"Complex_valued\")\n",
    "model_path = rootdir + 'notebooks/complex_nn_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 0.6984, Validation Loss: 0.7925\n",
      "Epoch [2/100], Training Loss: 0.8121, Validation Loss: 0.6998\n",
      "Epoch [3/100], Training Loss: 0.6048, Validation Loss: 0.6318\n",
      "Epoch [4/100], Training Loss: 0.4927, Validation Loss: 0.5816\n",
      "Epoch [5/100], Training Loss: 0.4166, Validation Loss: 0.5386\n",
      "Epoch [6/100], Training Loss: 0.6263, Validation Loss: 0.5050\n",
      "Epoch [7/100], Training Loss: 0.4879, Validation Loss: 0.4704\n",
      "Epoch [8/100], Training Loss: 0.4757, Validation Loss: 0.4452\n",
      "Epoch [9/100], Training Loss: 0.4301, Validation Loss: 0.4200\n",
      "Epoch [10/100], Training Loss: 0.4495, Validation Loss: 0.4009\n",
      "Epoch [11/100], Training Loss: 0.4736, Validation Loss: 0.3825\n",
      "Epoch [12/100], Training Loss: 0.2855, Validation Loss: 0.3652\n",
      "Epoch [13/100], Training Loss: 0.2906, Validation Loss: 0.3493\n",
      "Epoch [14/100], Training Loss: 0.2890, Validation Loss: 0.3352\n",
      "Epoch [15/100], Training Loss: 0.3835, Validation Loss: 0.3211\n",
      "Epoch [16/100], Training Loss: 0.2366, Validation Loss: 0.3098\n",
      "Epoch [17/100], Training Loss: 0.3034, Validation Loss: 0.2970\n",
      "Epoch [18/100], Training Loss: 0.2857, Validation Loss: 0.2855\n",
      "Epoch [19/100], Training Loss: 0.2114, Validation Loss: 0.2734\n",
      "Epoch [20/100], Training Loss: 0.2770, Validation Loss: 0.2658\n",
      "Epoch [21/100], Training Loss: 0.2591, Validation Loss: 0.2576\n",
      "Epoch [22/100], Training Loss: 0.2058, Validation Loss: 0.2473\n",
      "Epoch [23/100], Training Loss: 0.2517, Validation Loss: 0.2425\n",
      "Epoch [24/100], Training Loss: 0.1746, Validation Loss: 0.2362\n",
      "Epoch [25/100], Training Loss: 0.2166, Validation Loss: 0.2322\n",
      "Epoch [26/100], Training Loss: 0.1869, Validation Loss: 0.2195\n",
      "Epoch [27/100], Training Loss: 0.1909, Validation Loss: 0.2168\n",
      "Epoch [28/100], Training Loss: 0.3001, Validation Loss: 0.2079\n",
      "Epoch [29/100], Training Loss: 0.1977, Validation Loss: 0.2095\n",
      "Epoch [30/100], Training Loss: 0.1515, Validation Loss: 0.1981\n",
      "Epoch [31/100], Training Loss: 0.1819, Validation Loss: 0.1928\n",
      "Epoch [32/100], Training Loss: 0.1842, Validation Loss: 0.1911\n",
      "Epoch [33/100], Training Loss: 0.1797, Validation Loss: 0.1917\n",
      "Epoch [34/100], Training Loss: 0.1158, Validation Loss: 0.1821\n",
      "Epoch [35/100], Training Loss: 0.2343, Validation Loss: 0.1775\n",
      "Epoch [36/100], Training Loss: 0.1533, Validation Loss: 0.1740\n",
      "Epoch [37/100], Training Loss: 0.1472, Validation Loss: 0.1695\n",
      "Epoch [38/100], Training Loss: 0.1750, Validation Loss: 0.1666\n",
      "Epoch [39/100], Training Loss: 0.2145, Validation Loss: 0.1615\n",
      "Epoch [40/100], Training Loss: 0.2029, Validation Loss: 0.1581\n",
      "Epoch [41/100], Training Loss: 0.1793, Validation Loss: 0.1572\n",
      "Epoch [42/100], Training Loss: 0.1063, Validation Loss: 0.1536\n",
      "Epoch [43/100], Training Loss: 0.1292, Validation Loss: 0.1505\n",
      "Epoch [44/100], Training Loss: 0.1168, Validation Loss: 0.1466\n",
      "Epoch [45/100], Training Loss: 0.1415, Validation Loss: 0.1463\n",
      "Epoch [46/100], Training Loss: 0.1690, Validation Loss: 0.1437\n",
      "Epoch [47/100], Training Loss: 0.1125, Validation Loss: 0.1402\n",
      "Epoch [48/100], Training Loss: 0.0806, Validation Loss: 0.1405\n",
      "Epoch [49/100], Training Loss: 0.1253, Validation Loss: 0.1372\n",
      "Epoch [50/100], Training Loss: 0.1510, Validation Loss: 0.1362\n",
      "Epoch [51/100], Training Loss: 0.0803, Validation Loss: 0.1313\n",
      "Epoch [52/100], Training Loss: 0.1716, Validation Loss: 0.1292\n",
      "Epoch [53/100], Training Loss: 0.1591, Validation Loss: 0.1302\n",
      "Epoch [54/100], Training Loss: 0.1314, Validation Loss: 0.1262\n",
      "Epoch [55/100], Training Loss: 0.2201, Validation Loss: 0.1248\n",
      "Epoch [56/100], Training Loss: 0.1104, Validation Loss: 0.1228\n",
      "Epoch [57/100], Training Loss: 0.0870, Validation Loss: 0.1223\n",
      "Epoch [58/100], Training Loss: 0.1149, Validation Loss: 0.1181\n",
      "Epoch [59/100], Training Loss: 0.1231, Validation Loss: 0.1187\n",
      "Epoch [60/100], Training Loss: 0.1115, Validation Loss: 0.1170\n",
      "Epoch [61/100], Training Loss: 0.1155, Validation Loss: 0.1156\n",
      "Epoch [62/100], Training Loss: 0.0775, Validation Loss: 0.1151\n",
      "Epoch [63/100], Training Loss: 0.1027, Validation Loss: 0.1123\n",
      "Epoch [64/100], Training Loss: 0.0923, Validation Loss: 0.1111\n",
      "Epoch [65/100], Training Loss: 0.0853, Validation Loss: 0.1102\n",
      "Epoch [66/100], Training Loss: 0.1229, Validation Loss: 0.1085\n",
      "Epoch [67/100], Training Loss: 0.0824, Validation Loss: 0.1059\n",
      "Epoch [68/100], Training Loss: 0.1134, Validation Loss: 0.1057\n",
      "Epoch [69/100], Training Loss: 0.2168, Validation Loss: 0.1031\n",
      "Epoch [70/100], Training Loss: 0.1894, Validation Loss: 0.1033\n",
      "Epoch [71/100], Training Loss: 0.0897, Validation Loss: 0.1036\n",
      "Epoch [72/100], Training Loss: 0.0584, Validation Loss: 0.0993\n",
      "Epoch [73/100], Training Loss: 0.0832, Validation Loss: 0.0991\n",
      "Epoch [74/100], Training Loss: 0.0734, Validation Loss: 0.0977\n",
      "Epoch [75/100], Training Loss: 0.1552, Validation Loss: 0.0990\n",
      "Epoch [76/100], Training Loss: 0.1181, Validation Loss: 0.0954\n",
      "Epoch [77/100], Training Loss: 0.1411, Validation Loss: 0.0967\n",
      "Epoch [78/100], Training Loss: 0.1288, Validation Loss: 0.0942\n",
      "Epoch [79/100], Training Loss: 0.0439, Validation Loss: 0.0927\n",
      "Epoch [80/100], Training Loss: 0.0560, Validation Loss: 0.0948\n",
      "Epoch [81/100], Training Loss: 0.0793, Validation Loss: 0.0909\n",
      "Epoch [82/100], Training Loss: 0.1636, Validation Loss: 0.0933\n",
      "Epoch [83/100], Training Loss: 0.0824, Validation Loss: 0.0910\n",
      "Epoch [84/100], Training Loss: 0.1043, Validation Loss: 0.0883\n",
      "Epoch [85/100], Training Loss: 0.1393, Validation Loss: 0.0880\n",
      "Epoch [86/100], Training Loss: 0.2248, Validation Loss: 0.0902\n",
      "Epoch [87/100], Training Loss: 0.1444, Validation Loss: 0.0939\n",
      "Epoch [88/100], Training Loss: 0.1100, Validation Loss: 0.0908\n",
      "Epoch [89/100], Training Loss: 0.1000, Validation Loss: 0.0864\n",
      "Epoch [90/100], Training Loss: 0.0526, Validation Loss: 0.0853\n",
      "Epoch [91/100], Training Loss: 0.0389, Validation Loss: 0.0825\n",
      "Epoch [92/100], Training Loss: 0.0578, Validation Loss: 0.0860\n",
      "Epoch [93/100], Training Loss: 0.0439, Validation Loss: 0.0831\n",
      "Epoch [94/100], Training Loss: 0.0634, Validation Loss: 0.0833\n",
      "Epoch [95/100], Training Loss: 0.0466, Validation Loss: 0.0836\n",
      "Epoch [96/100], Training Loss: 0.0400, Validation Loss: 0.0820\n",
      "Epoch [97/100], Training Loss: 0.0897, Validation Loss: 0.0825\n",
      "Epoch [98/100], Training Loss: 0.0346, Validation Loss: 0.0817\n",
      "Epoch [99/100], Training Loss: 0.0637, Validation Loss: 0.0800\n",
      "Epoch [100/100], Training Loss: 0.0479, Validation Loss: 0.0793\n"
     ]
    }
   ],
   "source": [
    "if train_complex:\n",
    "    train(model, criterion, optimizer, scheduler, train_loader, test_loader, 100, model_path, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [-0.36573762 -0.8493817 ]\n",
      "Target: [-0.24728732 -0.75742614]\n",
      "\n",
      "\n",
      "Prediction: [ 1.0960218 -0.9100221]\n",
      "Target: [ 1.4674261 -0.8598122]\n",
      "\n",
      "\n",
      "Prediction: [ 0.28401774 -0.946801  ]\n",
      "Target: [-0.08344258 -0.94632244]\n",
      "\n",
      "\n",
      "Prediction: [ 0.34813672 -0.9221524 ]\n",
      "Target: [ 0.19683681 -0.8914559 ]\n",
      "\n",
      "\n",
      "Prediction: [-0.48367566 -0.10606176]\n",
      "Target: [-0.48456588 -0.10759523]\n",
      "\n",
      "\n",
      "Prediction: [-0.39170882  1.39896   ]\n",
      "Target: [-0.39610118  1.5324879 ]\n",
      "\n",
      "\n",
      "Prediction: [-0.0795102   0.71615255]\n",
      "Target: [0.316644 2.712674]\n",
      "\n",
      "\n",
      "Prediction: [-0.50579584 -0.36977682]\n",
      "Target: [-0.514367   -0.29206896]\n",
      "\n",
      "\n",
      "Prediction: [-0.4589251  1.2911007]\n",
      "Target: [-0.4434782  1.1970277]\n",
      "\n",
      "\n",
      "Prediction: [-0.44986913 -0.35354033]\n",
      "Target: [-0.5566556 -0.4382659]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if train_complex:\n",
    "    # Predictions from Complex model\n",
    "    # Load model\n",
    "    model = ComplexValuedNN()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Get 10 inputs\n",
    "    inputs, targets = next(iter(test_loader))\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    if type(outputs) == torch.Tensor:\n",
    "        outputs = outputs.cpu().detach().numpy()\n",
    "    if type(targets) == torch.Tensor:\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "    for i in range(0,10):\n",
    "        print(f\"Prediction: {outputs[i]}\")\n",
    "        print(f\"Target: {targets[i]}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Valued Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the Datasets so they output [x.real, x.imag] instead of x\n",
    "train_data = GlitchDataset(ifos, ml_models, datadict, glitches, distributions, NTRAIN)\n",
    "test_data = GlitchDataset(ifos, ml_models, datadict, glitches, distributions, NTEST)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=1024, shuffle=True, drop_last=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1024, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RealValuedNN(\n",
      "  (conv1_real): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (conv1_imag): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (activation1_real): ReLU()\n",
      "  (activation1_imag): ReLU()\n",
      "  (conv2_real): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=same)\n",
      "  (conv2_imag): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=same)\n",
      "  (activation2_real): ReLU()\n",
      "  (activation2_imag): ReLU()\n",
      "  (fc1_real): Linear(in_features=32832, out_features=128, bias=True)\n",
      "  (fc1_imag): Linear(in_features=32832, out_features=128, bias=True)\n",
      "  (activation3_real): ReLU()\n",
      "  (activation3_imag): ReLU()\n",
      "  (fc2_real): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (fc2_imag): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (activation4_real): ReLU()\n",
      "  (activation4_imag): ReLU()\n",
      "  (output_layer): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "Number of parameters: 8434498\n"
     ]
    }
   ],
   "source": [
    "input_dim = 513  # Number of input features\n",
    "output_dim = 2  # Number of output units (as specified in the dataset)\n",
    "\n",
    "# Create an instance of the network\n",
    "model = RealValuedNN()\n",
    "\n",
    "# put model on device\n",
    "model.to(device)\n",
    "\n",
    "# model = torch.compile(model)\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=20, verbose=True)\n",
    "writer = SummaryWriter(\"Real_valued\")\n",
    "model_path = rootdir + 'notebooks/real_nn_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 0.7921, Validation Loss: 0.8305\n",
      "Epoch [2/100], Training Loss: 0.6813, Validation Loss: 0.7392\n",
      "Epoch [3/100], Training Loss: 0.7610, Validation Loss: 0.6793\n",
      "Epoch [4/100], Training Loss: 0.5358, Validation Loss: 0.6317\n",
      "Epoch [5/100], Training Loss: 0.6132, Validation Loss: 0.5936\n",
      "Epoch [6/100], Training Loss: 0.5242, Validation Loss: 0.5638\n",
      "Epoch [7/100], Training Loss: 0.5790, Validation Loss: 0.5378\n",
      "Epoch [8/100], Training Loss: 0.7566, Validation Loss: 0.5163\n",
      "Epoch [9/100], Training Loss: 0.4142, Validation Loss: 0.4931\n",
      "Epoch [10/100], Training Loss: 0.5504, Validation Loss: 0.4744\n",
      "Epoch [11/100], Training Loss: 0.4549, Validation Loss: 0.4571\n",
      "Epoch [12/100], Training Loss: 0.3847, Validation Loss: 0.4407\n",
      "Epoch [13/100], Training Loss: 0.3523, Validation Loss: 0.4272\n",
      "Epoch [14/100], Training Loss: 0.3218, Validation Loss: 0.4125\n",
      "Epoch [15/100], Training Loss: 0.4525, Validation Loss: 0.4035\n",
      "Epoch [16/100], Training Loss: 0.4361, Validation Loss: 0.3886\n",
      "Epoch [17/100], Training Loss: 0.3307, Validation Loss: 0.3779\n",
      "Epoch [18/100], Training Loss: 0.3673, Validation Loss: 0.3672\n",
      "Epoch [19/100], Training Loss: 0.3848, Validation Loss: 0.3573\n",
      "Epoch [20/100], Training Loss: 0.2890, Validation Loss: 0.3482\n",
      "Epoch [21/100], Training Loss: 0.3639, Validation Loss: 0.3394\n",
      "Epoch [22/100], Training Loss: 0.3892, Validation Loss: 0.3309\n",
      "Epoch [23/100], Training Loss: 0.2534, Validation Loss: 0.3229\n",
      "Epoch [24/100], Training Loss: 0.3621, Validation Loss: 0.3163\n",
      "Epoch [25/100], Training Loss: 0.2081, Validation Loss: 0.3098\n",
      "Epoch [26/100], Training Loss: 0.3234, Validation Loss: 0.3023\n",
      "Epoch [27/100], Training Loss: 0.2207, Validation Loss: 0.2959\n",
      "Epoch [28/100], Training Loss: 0.2457, Validation Loss: 0.2896\n",
      "Epoch [29/100], Training Loss: 0.2198, Validation Loss: 0.2834\n",
      "Epoch [30/100], Training Loss: 0.2256, Validation Loss: 0.2780\n",
      "Epoch [31/100], Training Loss: 0.2174, Validation Loss: 0.2730\n",
      "Epoch [32/100], Training Loss: 0.2802, Validation Loss: 0.2686\n",
      "Epoch [33/100], Training Loss: 0.2061, Validation Loss: 0.2628\n",
      "Epoch [34/100], Training Loss: 0.2678, Validation Loss: 0.2573\n",
      "Epoch [35/100], Training Loss: 0.2065, Validation Loss: 0.2532\n",
      "Epoch [36/100], Training Loss: 0.2131, Validation Loss: 0.2487\n",
      "Epoch [37/100], Training Loss: 0.2445, Validation Loss: 0.2442\n",
      "Epoch [38/100], Training Loss: 0.1937, Validation Loss: 0.2419\n",
      "Epoch [39/100], Training Loss: 0.2164, Validation Loss: 0.2397\n",
      "Epoch [40/100], Training Loss: 0.2672, Validation Loss: 0.2330\n",
      "Epoch [41/100], Training Loss: 0.2607, Validation Loss: 0.2304\n",
      "Epoch [42/100], Training Loss: 0.2060, Validation Loss: 0.2248\n",
      "Epoch [43/100], Training Loss: 0.2078, Validation Loss: 0.2218\n",
      "Epoch [44/100], Training Loss: 0.2082, Validation Loss: 0.2191\n",
      "Epoch [45/100], Training Loss: 0.3278, Validation Loss: 0.2186\n",
      "Epoch [46/100], Training Loss: 0.2187, Validation Loss: 0.2155\n",
      "Epoch [47/100], Training Loss: 0.2452, Validation Loss: 0.2101\n",
      "Epoch [48/100], Training Loss: 0.1933, Validation Loss: 0.2080\n",
      "Epoch [49/100], Training Loss: 0.2608, Validation Loss: 0.2077\n",
      "Epoch [50/100], Training Loss: 0.2347, Validation Loss: 0.2029\n",
      "Epoch [51/100], Training Loss: 0.1799, Validation Loss: 0.1988\n",
      "Epoch [52/100], Training Loss: 0.1490, Validation Loss: 0.1937\n",
      "Epoch [53/100], Training Loss: 0.1563, Validation Loss: 0.1925\n",
      "Epoch [54/100], Training Loss: 0.1613, Validation Loss: 0.1903\n",
      "Epoch [55/100], Training Loss: 0.1937, Validation Loss: 0.1875\n",
      "Epoch [56/100], Training Loss: 0.1930, Validation Loss: 0.1854\n",
      "Epoch [57/100], Training Loss: 0.1510, Validation Loss: 0.1824\n",
      "Epoch [58/100], Training Loss: 0.1832, Validation Loss: 0.1883\n",
      "Epoch [59/100], Training Loss: 0.1531, Validation Loss: 0.1805\n",
      "Epoch [60/100], Training Loss: 0.2590, Validation Loss: 0.1785\n",
      "Epoch [61/100], Training Loss: 0.1514, Validation Loss: 0.1751\n",
      "Epoch [62/100], Training Loss: 0.1748, Validation Loss: 0.1734\n",
      "Epoch [63/100], Training Loss: 0.1695, Validation Loss: 0.1703\n",
      "Epoch [64/100], Training Loss: 0.1217, Validation Loss: 0.1700\n",
      "Epoch [65/100], Training Loss: 0.1289, Validation Loss: 0.1677\n",
      "Epoch [66/100], Training Loss: 0.1906, Validation Loss: 0.1662\n",
      "Epoch [67/100], Training Loss: 0.1800, Validation Loss: 0.1660\n",
      "Epoch [68/100], Training Loss: 0.1867, Validation Loss: 0.1624\n",
      "Epoch [69/100], Training Loss: 0.1662, Validation Loss: 0.1623\n",
      "Epoch [70/100], Training Loss: 0.1932, Validation Loss: 0.1587\n",
      "Epoch [71/100], Training Loss: 0.2086, Validation Loss: 0.1601\n",
      "Epoch [72/100], Training Loss: 0.2296, Validation Loss: 0.1549\n",
      "Epoch [73/100], Training Loss: 0.1309, Validation Loss: 0.1546\n",
      "Epoch [74/100], Training Loss: 0.1443, Validation Loss: 0.1554\n",
      "Epoch [75/100], Training Loss: 0.1465, Validation Loss: 0.1534\n",
      "Epoch [76/100], Training Loss: 0.1145, Validation Loss: 0.1509\n",
      "Epoch [77/100], Training Loss: 0.1291, Validation Loss: 0.1509\n",
      "Epoch [78/100], Training Loss: 0.1404, Validation Loss: 0.1485\n",
      "Epoch [79/100], Training Loss: 0.1264, Validation Loss: 0.1467\n",
      "Epoch [80/100], Training Loss: 0.2059, Validation Loss: 0.1449\n",
      "Epoch [81/100], Training Loss: 0.1214, Validation Loss: 0.1440\n",
      "Epoch [82/100], Training Loss: 0.1258, Validation Loss: 0.1416\n",
      "Epoch [83/100], Training Loss: 0.1343, Validation Loss: 0.1402\n",
      "Epoch [84/100], Training Loss: 0.1008, Validation Loss: 0.1396\n",
      "Epoch [85/100], Training Loss: 0.0947, Validation Loss: 0.1378\n",
      "Epoch [86/100], Training Loss: 0.1401, Validation Loss: 0.1353\n",
      "Epoch [87/100], Training Loss: 0.1656, Validation Loss: 0.1357\n",
      "Epoch [88/100], Training Loss: 0.1280, Validation Loss: 0.1334\n",
      "Epoch [89/100], Training Loss: 0.1506, Validation Loss: 0.1371\n",
      "Epoch [90/100], Training Loss: 0.1476, Validation Loss: 0.1328\n",
      "Epoch [91/100], Training Loss: 0.1467, Validation Loss: 0.1302\n",
      "Epoch [92/100], Training Loss: 0.0876, Validation Loss: 0.1327\n",
      "Epoch [93/100], Training Loss: 0.1251, Validation Loss: 0.1296\n",
      "Epoch [94/100], Training Loss: 0.1147, Validation Loss: 0.1308\n",
      "Epoch [95/100], Training Loss: 0.1159, Validation Loss: 0.1279\n",
      "Epoch [96/100], Training Loss: 0.1258, Validation Loss: 0.1285\n",
      "Epoch [97/100], Training Loss: 0.0868, Validation Loss: 0.1290\n",
      "Epoch [98/100], Training Loss: 0.1179, Validation Loss: 0.1248\n",
      "Epoch [99/100], Training Loss: 0.0985, Validation Loss: 0.1268\n",
      "Epoch [100/100], Training Loss: 0.1177, Validation Loss: 0.1283\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "if train_real:\n",
    "    train(model, criterion, optimizer, scheduler, train_loader, test_loader, 100, model_path, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [-0.38858858 -0.9720913 ]\n",
      "Target: [-0.24728732 -0.75742614]\n",
      "\n",
      "\n",
      "Prediction: [ 1.4360641  -0.75076455]\n",
      "Target: [ 1.4674261 -0.8598122]\n",
      "\n",
      "\n",
      "Prediction: [ 0.23093775 -0.9771106 ]\n",
      "Target: [-0.08344258 -0.94632244]\n",
      "\n",
      "\n",
      "Prediction: [ 0.55242974 -0.82873106]\n",
      "Target: [ 0.19683681 -0.8914559 ]\n",
      "\n",
      "\n",
      "Prediction: [-0.48066345 -0.22690825]\n",
      "Target: [-0.48456588 -0.10759523]\n",
      "\n",
      "\n",
      "Prediction: [-0.44693607  1.2371559 ]\n",
      "Target: [-0.39610118  1.5324879 ]\n",
      "\n",
      "\n",
      "Prediction: [0.02147465 0.8863181 ]\n",
      "Target: [0.316644 2.712674]\n",
      "\n",
      "\n",
      "Prediction: [-0.39063573 -0.00678147]\n",
      "Target: [-0.514367   -0.29206896]\n",
      "\n",
      "\n",
      "Prediction: [-0.46402672  1.2403691 ]\n",
      "Target: [-0.4434782  1.1970277]\n",
      "\n",
      "\n",
      "Prediction: [-0.3967832   0.01413908]\n",
      "Target: [-0.5566556 -0.4382659]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if train_real:\n",
    "    # Predictions from Real model\n",
    "    # Load model\n",
    "    model = RealValuedNN()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Get 10 inputs\n",
    "    inputs, targets = next(iter(test_loader))\n",
    "    outputs = model(inputs.real, inputs.imag)\n",
    "\n",
    "    if type(outputs) == torch.Tensor:\n",
    "        outputs = outputs.cpu().detach().numpy()\n",
    "    if type(targets) == torch.Tensor:\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "    for i in range(0,10):\n",
    "        print(f\"Prediction: {outputs[i]}\")\n",
    "        print(f\"Target: {targets[i]}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "antiglitch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
