{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVNN Playground\n",
    "\n",
    "The purpose of this notebook is to provide a simple playground for the Complex-Valued Neural Networks (CVNN) implementation, in the aim of proving their strengths vs real-valued neural networks. The CVNN is a neural network that uses complex numbers as input, output, and weights. The CVNN is implemented using PyTorch and the complextorch library.  \n",
    "  \n",
    "The notebook calls to a couple of other python files:  \n",
    "- `cvnn_data.py` contains the data generation functions.  \n",
    "- `cvnn_model.py` contains the CVNN model definitions.  \n",
    "\n",
    "The notebook is divided into the following sections:\n",
    "1. **Imports and settings** - Importing the necessary libraries and setting the data paths etc.\n",
    "2. **Data Preparation** - Generating the data for the CVNN. By default this adds noise to the data, randomly phase shifts the data by either -pi, 0, or pi, and randomly shifts the signal in time by a value between -0.1s and 0.1s.\n",
    "3. **Training loop definition**.\n",
    "4. **Complex Valued Training** - Training `ComplexValuedNN` defined in `cvnn_models.py` on the generated data. This model uses two 1d complex convolutional layers with fully complex activation functions (Cardoid) and two fully connected layers.  \n",
    "5. **Real Valued Training** - Training `RealValuedNN` defined in `cvnn_models.py` on the generated data. This model uses two 1d real convolutional layers (real and imag in a channel each) with ReLU activation functions and two fully connected layers.\n",
    "\n",
    "### Results:  \n",
    "Conv layers are required? \n",
    "It seems CVNNs can learn better than RVNNs when turning the number of conv filters down so the network goes <1m parameters.  \n",
    "\n",
    "Plot of the loss and accuracy of the CVNN and RVNN models during training:\n",
    "![lossoutput_1m_pars.png](../cvnn_lossoutput_1m_pars.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.fft import rfft, irfft\n",
    "\n",
    "from functools import partial\n",
    "rfft = partial(rfft, norm='ortho')\n",
    "irfft = partial(irfft, norm='ortho')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Sampler\n",
    "from typing import Iterator, Sized, List\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from cvnn_models import ComplexValuedNN, RealValuedNN\n",
    "from cvnn_data import get_data, GlitchDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from antiglitch import SnippetNormed\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch setup\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "def memsum():\n",
    "    \"\"\"Prints a summary of the GPU memory usage.\"\"\"\n",
    "    print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switches to control training\n",
    "train_complex, test_complex = True, True\n",
    "train_real, test_real = True, True\n",
    "train_real_td, test_real_td = False, False\n",
    "\n",
    "# define dataset\n",
    "NTRAIN = 100000\n",
    "NTEST = 50000\n",
    "NBATCH = 4096\n",
    "\n",
    "rootdir = '/home/xangma/OneDrive/repos/antiglitch/'\n",
    "datadir = rootdir + 'data/'\n",
    "\n",
    "# set seed for data generation - noise, augmentation, etc.\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from datadir\n",
    "distributions, glitches, ifos, ml_models = get_data(datadir)\n",
    "print(f\"Loaded {len(distributions)} distributions, {len(glitches)} glitches, {ifos} ifos, and {ml_models} ml_models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and test datasets\n",
    "train_data = GlitchDataset(datadir, ifos, ml_models, glitches, distributions, NTRAIN, NTEST, device, True, True, True, 'train', 'complex', None)\n",
    "test_data = GlitchDataset(datadir, ifos, ml_models, glitches, distributions, NTRAIN, NTEST, device, True, True, True, 'test', 'complex', train_data)\n",
    "\n",
    "# # print max and mins of training and test data\n",
    "print(f\"Train data real + imag maxs: {torch.max(train_data.x_arr.real)}, {torch.max(train_data.x_arr.imag)}\")\n",
    "print(f\"Test data real + imag maxs: {torch.max(test_data.x_arr.real)}, {torch.max(test_data.x_arr.imag)}\")\n",
    "print(f\"Train data real + imag mins: {torch.min(train_data.x_arr.real)}, {torch.min(train_data.x_arr.imag)}\")\n",
    "print(f\"Test data real + imag mins: {torch.min(test_data.x_arr.real)}, {torch.min(test_data.x_arr.imag)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some data\n",
    "\n",
    "trainitem = train_data.__getitem__(0)\n",
    "testitem = test_data.__getitem__(0)\n",
    "trainitem=trainitem[0].cpu().numpy()\n",
    "testitem=testitem[0].cpu().numpy()\n",
    "\n",
    "# Get a real snippet\n",
    "ifo = np.random.choice(ifos)\n",
    "ml_model = np.random.choice(ml_models)\n",
    "glitch_num = random.choice(glitches[ifo][ml_model])\n",
    "glitch_num = glitch_num['num']\n",
    "# create a SnippetNormed object\n",
    "snip = SnippetNormed(ifo, ml_model, glitch_num, datadir)\n",
    "inf = {}\n",
    "inf['freqs'] = np.linspace(0, 4096, 513)\n",
    "snip.set_infer(inf)\n",
    "# get the glitch data in the frequency domain\n",
    "actual_item = rfft(snip.whts)\n",
    "# scale it by the training scalings\n",
    "actual_item_real = (actual_item.real - train_data.tr_x_mean_real) / train_data.tr_x_std_real\n",
    "actual_item_imag = (actual_item.imag - train_data.tr_x_mean_imag) / train_data.tr_x_std_imag\n",
    "actual_item = actual_item_real + 1j * actual_item_imag\n",
    "actual_item = np.abs(actual_item)\n",
    "# plot them all side by side\n",
    "plt.figure()\n",
    "plt.subplot(1,3,1)\n",
    "plt.loglog(np.abs(trainitem))\n",
    "plt.title('Train')\n",
    "plt.subplot(1,3,2)\n",
    "plt.loglog(np.abs(testitem))\n",
    "plt.title('Test')\n",
    "plt.subplot(1,3,3)\n",
    "plt.loglog(actual_item)\n",
    "plt.title('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastRandomSampler(Sampler[int]):\n",
    "    def __init__(self, data_source: Sized, batch_size: int, generator=None) -> None:\n",
    "        self.data_source = data_source\n",
    "        self.data_len = len(self.data_source)\n",
    "        self.batch_size = batch_size\n",
    "        self.generator = generator or torch.Generator()\n",
    "        self.epoch_indices = []\n",
    "        self.set_epoch(0)  # Shuffle at initialization\n",
    "\n",
    "    def _shuffle_indices(self):\n",
    "        self.epoch_indices = torch.randperm(self.data_len, generator=self.generator).tolist()\n",
    "\n",
    "    def __iter__(self) -> Iterator[List[int]]:\n",
    "        # Yield batches of indices using slicing\n",
    "        return (self.epoch_indices[i:i + self.batch_size] for i in range(0, self.data_len, self.batch_size))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # Number of batches in an epoch\n",
    "        return (len(self.data_source) + self.batch_size - 1) // self.batch_size\n",
    "    \n",
    "    def set_epoch(self, epoch: int) -> None:\n",
    "        self._shuffle_indices()  # Shuffle indices for the new epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into DataLoader\n",
    "tr_sampler = FastRandomSampler(train_data, NBATCH)\n",
    "te_sampler = FastRandomSampler(test_data, NBATCH)\n",
    "collate_fn = lambda x: tuple(x)\n",
    "train_loader = DataLoader(dataset=train_data, sampler=tr_sampler,   drop_last=False)\n",
    "test_loader = DataLoader(dataset=test_data, sampler=te_sampler,   drop_last=False)\n",
    "list(train_loader.__iter__())[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, criterion, optimizer, scheduler, train_loader, test_loader, num_epochs, model_path, writer):\n",
    "    epoch_count=0\n",
    "    # Load model if it exists\n",
    "    try:\n",
    "        # find model with highest epoch number\n",
    "        modelfn = model_path.split('/')[-1]\n",
    "        modeldir = model_path.split('/')[:-1].join('/')\n",
    "        model_files = os.listdir(modeldir)\n",
    "        model_files = [f for f in model_files if f.endswith('.pt') and f.startswith(modelfn.split('.')[0])]\n",
    "        if len(model_files) > 0:\n",
    "            model_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "            model_path = model_path + model_files[-1]\n",
    "            print(f'Loading model {model_path}')\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            print('Model loaded')\n",
    "            epoch_count = int(model_path.split('_')[-1].split('.')[0])\n",
    "    except:\n",
    "        print('Model not loaded')\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    loss_arr = []\n",
    "    val_loss_arr = []\n",
    "    while epoch_count < num_epochs:\n",
    "        model.train()\n",
    "        total_training_loss = 0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, targets = data\n",
    "            # Forward pass\n",
    "            outputs = model(inputs[0])\n",
    "            loss = criterion(outputs, targets[0])\n",
    "            total_training_loss += loss.item()\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        loss_arr.append(total_training_loss/len(train_loader))\n",
    "\n",
    "        # Test the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            for i, data in enumerate(test_loader):\n",
    "                inputs, targets = data\n",
    "                outputs = model(inputs[0])\n",
    "                val_loss = criterion(outputs, targets[0]).item()\n",
    "                total_loss += val_loss\n",
    "            val_loss_arr.append(total_loss/len(test_loader))\n",
    "        print(f\"Epoch [{epoch_count+1}/{num_epochs}], Training Loss: {loss.item():.4f}, Validation Loss: {total_loss/len(test_loader):.4f}\")\n",
    "        \n",
    "        # update the learning rate\n",
    "        scheduler.step(total_loss/len(test_loader))\n",
    "\n",
    "        train_loader.sampler.set_epoch(epoch_count)\n",
    "        epoch_count += 1\n",
    "        \n",
    "        writer.add_scalars(\"Loss\", {\"Train\": total_training_loss/len(train_loader), \"Validation\": total_loss/len(test_loader)}, epoch_count)\n",
    "        writer.add_scalar(\"Learning rate\", optimizer.param_groups[0]['lr'], epoch_count)\n",
    "        \n",
    "        # save the model every 10 epochs\n",
    "        if epoch_count % 10 == 0 and epoch_count != 0:\n",
    "            torch.save(model.state_dict(), model_path.split('.')[0] + f'_{epoch_count}.pt')\n",
    "        writer.flush()\n",
    "    return loss_arr, val_loss_arr\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Valued Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_complex:\n",
    "    if any([x in locals() for x in ['model', 'criterion', 'optimizer', 'scheduler', 'writer']]):\n",
    "        del model, criterion, optimizer, scheduler, writer\n",
    "        torch.cuda.empty_cache()\n",
    "    # Create an instance of the network\n",
    "    def get_complex_model():\n",
    "        return ComplexValuedNN(n_conv_layers=2, conv_filters=[4,8], conv_kernel_size=[1,3], n_fc_layers=1, n_fc_units=128)\n",
    "    \n",
    "    model = get_complex_model()\n",
    "    # put model on device\n",
    "    model.to(device)\n",
    "\n",
    "    model = torch.compile(model)\n",
    "    # Print the model architecture\n",
    "    print(model)\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=20, verbose=True)\n",
    "    writer = SummaryWriter(\"Complex_valued\")\n",
    "    model_path = rootdir + 'notebooks/complex_nn_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "if train_complex:\n",
    "    complex_train_loss_arr,  complex_val_loss_arr = train(model, criterion, optimizer, scheduler, train_loader, test_loader, 100, model_path, writer)\n",
    "    # save the loss arrays into a single file\n",
    "    np.savez(rootdir + 'notebooks/complex_loss_arrays.npz', train=complex_train_loss_arr, val=complex_val_loss_arr)\n",
    "    del model, criterion, optimizer, scheduler, writer\n",
    "    torch.cuda.empty_cache()\n",
    "    memsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_complex:\n",
    "    # Predictions from Complex model\n",
    "    # Load model\n",
    "    model = get_complex_model()\n",
    "    model = torch.compile(model)\n",
    "    \n",
    "    modelfn = model_path.split('/')[-1]\n",
    "    modeldir = '/'.join(model_path.split('/')[:-1])\n",
    "    model_files = os.listdir(modeldir)\n",
    "    model_files = [f for f in model_files if f.endswith('.pt') and f.startswith(modelfn.split('.')[0])]\n",
    "    if len(model_files) > 0:\n",
    "        model_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "        model_path = modeldir + '/' + model_files[-1]\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_complex:\n",
    "    # Get 10 inputs\n",
    "    inputs, targets = next(iter(test_loader))\n",
    "    outputs = model(inputs[0])\n",
    "\n",
    "    if type(outputs) == torch.Tensor:\n",
    "        outputs = outputs.cpu().detach().numpy()\n",
    "    if type(targets) == torch.Tensor:\n",
    "        targets = targets[0].cpu().detach().numpy()\n",
    "    for i in range(0,10):\n",
    "        print(f\"Prediction: {outputs[i]}\")\n",
    "        print(f\"Target: {targets[i]}\")\n",
    "        print(\"\\n\")\n",
    "    del model, inputs, targets, outputs\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Valued Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_real:\n",
    "    # Create an instance of the network\n",
    "    def get_real_model():\n",
    "        return RealValuedNN(n_conv_layers=2, conv_filters=[8,16], conv_kernel_size=[1,3], n_fc_layers=1, n_fc_units=128)\n",
    "\n",
    "    # put model on device\n",
    "    model = get_real_model()\n",
    "    model.to(device)\n",
    "    model = torch.compile(model)\n",
    "\n",
    "    # Print the model architecture\n",
    "    print(model)\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=20, verbose=True)\n",
    "    writer = SummaryWriter(\"Real_valued\")\n",
    "    model_path = rootdir + 'notebooks/real_nn_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "if train_real:\n",
    "    real_train_loss_arr,  real_val_loss_arr = train(model, criterion, optimizer, scheduler, train_loader, test_loader, 100, model_path, writer)\n",
    "    # save the loss arrays into a single file\n",
    "    np.savez(rootdir + 'notebooks/real_loss_arrays.npz', train=real_train_loss_arr, val=real_val_loss_arr)\n",
    "    del model, criterion, optimizer, scheduler, writer\n",
    "    torch.cuda.empty_cache()\n",
    "    memsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_real:\n",
    "    # Predictions from Real model\n",
    "    # Load model\n",
    "    model = get_real_model()\n",
    "    model = torch.compile(model)\n",
    "    \n",
    "    modelfn = model_path.split('/')[-1]\n",
    "    modeldir = '/'.join(model_path.split('/')[:-1])\n",
    "    model_files = os.listdir(modeldir)\n",
    "    model_files = [f for f in model_files if f.endswith('.pt') and f.startswith(modelfn.split('.')[0])]\n",
    "    if len(model_files) > 0:\n",
    "        model_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "        model_path = modeldir + '/' + model_files[-1]\n",
    "        \n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_real:\n",
    "    # Get 10 inputs\n",
    "    inputs, targets = next(iter(test_loader))\n",
    "    outputs = model(inputs[0])\n",
    "\n",
    "    if type(outputs) == torch.Tensor:\n",
    "        outputs = outputs.cpu().detach().numpy()\n",
    "    if type(targets) == torch.Tensor:\n",
    "        targets = targets[0].cpu().detach().numpy()\n",
    "    for i in range(0,10):\n",
    "        print(f\"Prediction: {outputs[i]}\")\n",
    "        print(f\"Target: {targets[i]}\")\n",
    "        print(\"\\n\")\n",
    "    del model, inputs, targets, outputs\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load any existing complex and real loss arrays\n",
    "if 'complex_train_loss_arr' not in locals():\n",
    "    complex_loss_arrays = np.load(rootdir + 'notebooks/complex_loss_arrays.npz')\n",
    "    complex_train_loss_arr = complex_loss_arrays['train']\n",
    "    complex_val_loss_arr = complex_loss_arrays['val']\n",
    "if 'real_train_loss_arr' not in locals():\n",
    "    real_loss_arrays = np.load(rootdir + 'notebooks/real_loss_arrays.npz')\n",
    "    real_train_loss_arr = real_loss_arrays['train']\n",
    "    real_val_loss_arr = real_loss_arrays['val']\n",
    "\n",
    "# Plot the loss arrays\n",
    "plt.figure()\n",
    "plt.plot(complex_train_loss_arr, label='Complex Train')\n",
    "plt.plot(complex_val_loss_arr, label='Complex Validation')\n",
    "plt.plot(real_train_loss_arr, label='Real Train')\n",
    "plt.plot(real_val_loss_arr, label='Real Validation')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Valued in the time domain ( DEV/UNTESTED CODE BELOW )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_data, test_data, train_loader, test_loader\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "train_data = GlitchDataset(datadir, ifos, ml_models, glitches, distributions, NTRAIN, NTEST, device, True, 'train', 'real')\n",
    "test_data = GlitchDataset(datadir, ifos, ml_models, glitches, distributions, NTRAIN, NTEST, device, True, 'test', 'real')\n",
    "tr_sampler = FastRandomSampler(train_data, 1024)\n",
    "te_sampler = FastRandomSampler(test_data, 1024)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data,sampler = tr_sampler,  drop_last=False)\n",
    "test_loader = DataLoader(dataset=test_data, sampler = te_sampler, drop_last=False)\n",
    "# testitem = test_data.__getitem__(0)\n",
    "# print(testitem[0][1:20], testitem[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from antiglitch import SnippetNormed\n",
    "trainitem = train_data.__getitem__(np.random.randint(0, NTRAIN))\n",
    "testitem = test_data.__getitem__(np.random.randint(0, NTEST))\n",
    "\n",
    "# Get a real snippet\n",
    "ifo = np.random.choice(ifos)\n",
    "ml_model = np.random.choice(ml_models)\n",
    "glitch_num = random.choice(glitches[ifo][ml_model])\n",
    "glitch_num = glitch_num['num']\n",
    "# create a SnippetNormed object\n",
    "snip = SnippetNormed(ifo, ml_model, glitch_num, datadir)\n",
    "inf = {}\n",
    "inf['freqs'] = np.linspace(0, 4096, 513)\n",
    "snip.set_infer(inf)\n",
    "# get the glitch data in the time domain\n",
    "actual_item = snip.whts\n",
    "# scale it by the training scalings\n",
    "actual_item = (actual_item - train_data.x_arr_mean) / train_data.x_arr_std\n",
    "# plot them all side by side\n",
    "plt.figure()\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(trainitem[0].cpu().numpy())\n",
    "plt.title('Train')\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(testitem[0].cpu().numpy())\n",
    "plt.title('Test')\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(actual_item)\n",
    "plt.title('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real Valued Training\n",
    "input_dim = 513  # Number of input features\n",
    "output_dim = 2  # Number of output units (as specified in the dataset)\n",
    "# reimport the real valued model from cvnn_models\n",
    "from cvnn_models import RealValuedNNtd\n",
    "\n",
    "# Create an instance of the network\n",
    "model = RealValuedNNtd()\n",
    "\n",
    "# put model on device\n",
    "model.to(device)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=20, verbose=True)\n",
    "writer = SummaryWriter(\"Real_valued_td\")\n",
    "model_path = rootdir + 'notebooks/real_td_nn_model.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "if train_real_td:\n",
    "    train(model, criterion, optimizer, scheduler, train_loader, test_loader, 100, model_path, writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1024*16384)-(65536*256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del model, criterion, optimizer, scheduler, writer\n",
    "torch.cuda.empty_cache()\n",
    "memsum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if test_real_td:\n",
    "    # Predictions from Real model\n",
    "    # Load model\n",
    "    model = RealValuedNN()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if test_real_td:\n",
    "    # Get 10 inputs\n",
    "    inputs, targets = next(iter(test_loader))\n",
    "    outputs = model(inputs[0].real, inputs[0].imag)\n",
    "\n",
    "    if type(outputs) == torch.Tensor:\n",
    "        outputs = outputs.cpu().detach().numpy()\n",
    "    if type(targets) == torch.Tensor:\n",
    "        targets = targets[0].cpu().detach().numpy()\n",
    "    for i in range(0,10):\n",
    "        print(f\"Prediction: {outputs[i]}\")\n",
    "        print(f\"Target: {targets[i]}\")\n",
    "        print(\"\\n\")\n",
    "del model, inputs, targets, outputs\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "antiglitch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
