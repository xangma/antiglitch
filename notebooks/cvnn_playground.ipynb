{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import complextorch as cvtorch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import re\n",
    "\n",
    "import antiglitch\n",
    "from antiglitch import to_fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/home/xangma/OneDrive/repos/antiglitch/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# files are named as follows: ifo-key-num.npz,\n",
    "# e.g. L1-lowblip-0355.npz\n",
    "\n",
    "# Find all ifos, keys, and numbers in the data directory\n",
    "ifos = []\n",
    "ml_models = []\n",
    "numbers = []\n",
    "\n",
    "# filename pattern: ifo-key-num.npz\n",
    "filename_pattern = re.compile(r'([A-Z0-9]+)-([a-z]+)-(\\d+).npz')\n",
    "\n",
    "for file in os.listdir(datadir):\n",
    "\t# Get unique ifos:\n",
    "\ttry:\n",
    "\t\t# check filename format\n",
    "\t\tif filename_pattern.match(file):\n",
    "\t\t\tifo = file.split('-')[0]\n",
    "\t\t\tif ifo not in ifos:\n",
    "\t\t\t\tifos.append(ifo)\n",
    "\t\t\t# Get unique keys:\n",
    "\t\t\tml_model = file.split('-')[1]\n",
    "\t\t\tif ml_model not in ml_models:\n",
    "\t\t\t\tml_models.append(ml_model)\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\n",
    "datadict={}\n",
    "for ifo in ifos:\n",
    "\tdatadict[ifo] = {}\n",
    "\tfor ml_model in ml_models:\n",
    "\t\tdatadict[ifo][ml_model] = []\n",
    "\t\tfor file in os.listdir(datadir):\n",
    "\t\t\ttry:\n",
    "\t\t\t\tifo_file = file.split('-')[0]\n",
    "\t\t\t\tkey_file = file.split('-')[1]\n",
    "\t\t\t\tnum = file.split('-')[2].split('.')[0]\n",
    "\t\t\t\tif ifo_file == ifo and key_file == ml_model:\n",
    "\t\t\t\t\tdatadict[ifo][ml_model].append(int(num))\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results\n",
    "resultsjson=datadir + 'all_PE_v3.json'\n",
    "with open(resultsjson, 'r') as f:\n",
    "\tresults = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xangma/OneDrive/repos/antiglitch/antiglitch/utils.py:29: RuntimeWarning: divide by zero encountered in power\n",
      "  invasd = ((4096.*npz['psd'])**-0.5)[:4097]\n"
     ]
    }
   ],
   "source": [
    "# load all glitches into an array using the datadict and Snippet class\n",
    "glitches = {}\n",
    "for ifo in ifos:\n",
    "    if ifo not in glitches:\n",
    "        glitches[ifo] = {}\n",
    "    for ml_model in ml_models:\n",
    "        if ml_model not in glitches[ifo]:\n",
    "            glitches[ifo][ml_model] = {}\n",
    "        for num in datadict[ifo][ml_model]:\n",
    "            # try:\n",
    "                if num not in glitches[ifo][ml_model]:\n",
    "                    glitches[ifo][ml_model][num] = {}\n",
    "                snip = antiglitch.SnippetNormed(ifo, ml_model, num, datadir)\n",
    "                glitches[ifo][ml_model][num]['data'] = to_fd(snip.whts)\n",
    "                glitches[ifo][ml_model][num]['invasd'] = snip.invasd\n",
    "            # except:\n",
    "            #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_label_map = {'Blip_Low_Frequency':'lowblip', 'Blip':'blip', 'Koi_Fish':'koi', 'Tomte':'tomte'}\n",
    "results_keys = ['f0', 'f0_sd', 'gbw', 'gbw_sd', 'amp_r', 'amp_r_sd', 'amp_i', 'amp_i_sd', 'time', 'time_sd', 'num']\n",
    "\n",
    "nums = list(results['num'].values())\n",
    "for i in range(len(nums)):\n",
    "    try:\n",
    "        stri = str(i)\n",
    "        ifo = results['ifo'][stri]\n",
    "        ml_label = ml_label_map[results['ml_label'][stri]]\n",
    "        \n",
    "        for key in results_keys:\n",
    "            glitches[ifo][ml_label][nums[i]][key] = results[key][stri]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Restructure the dataset (Yes I know it was originally in this format, you leave me alone)\n",
    "distributions = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "\n",
    "for ifo in glitches:\n",
    "    for ml_model in glitches[ifo]:\n",
    "        for glitch_num in glitches[ifo][ml_model]:\n",
    "            for key in results_keys:\n",
    "                distributions[ifo][ml_model][key].append(glitches[ifo][ml_model][glitch_num][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sample(ifo, ml_model, tosample=['f0', 'gbw', 'amp_r', 'amp_i', 'time']):\n",
    "    res = {key:None for key in tosample}\n",
    "    for key in tosample:\n",
    "        draw = np.random.choice(distributions[ifo][ml_model][key])\n",
    "        draw_sd = distributions[ifo][ml_model][key + '_sd'][distributions[ifo][ml_model][key].index(draw)]\n",
    "        draw_final = np.random.normal(draw, draw_sd)\n",
    "        res[key] = draw_final\n",
    "    return res\n",
    "\n",
    "def new_init_Snippet(self, invasd):\n",
    "        self.invasd = invasd\n",
    "\n",
    "SnippetNormed = type('SnippetNormed', (antiglitch.SnippetNormed,), {'__init__': new_init_Snippet})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snip(ifos, ml_models, tosample=['f0', 'gbw', 'amp_r', 'amp_i', 'time']):\n",
    "\t\tifo = np.random.choice(ifos)\n",
    "\t\tml_model = np.random.choice(ml_models)\n",
    "\t\tglitch_num = random.choice(datadict[ifo][ml_model])\n",
    "\t\tglitch_invasd = glitches[ifo][ml_model][glitch_num]['invasd']\n",
    "\t\tsnip = SnippetNormed(glitch_invasd)\n",
    "\t\tinf = gen_sample(ifo, ml_model)\n",
    "\t\tinf['freqs'] = snip.invasd\n",
    "\t\tsnip.set_infer(inf)\n",
    "\t\tx = snip.fglitch\n",
    "\t\t# check for nans\n",
    "\t\tif np.isnan(x).any():\n",
    "\t\t\treturn get_snip(ifos, ml_models, tosample)\n",
    "\t\ty = (snip.inf['f0'], snip.inf['gbw'])\n",
    "\t\treturn x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0036-3.9260e-04j, -0.1319-7.6944e-02j, -0.6444-4.6743e-01j,\n",
      "        -1.2034-1.0866e+00j, -1.3743-1.5321e+00j, -1.0950-1.5078e+00j,\n",
      "        -0.7373-1.2664e+00j, -0.4926-1.0779e+00j, -0.2991-8.6897e-01j,\n",
      "        -0.1503-6.2775e-01j, -0.0614-4.4096e-01j, -0.0128-3.1188e-01j,\n",
      "         0.0128-2.2166e-01j,  0.0259-1.6265e-01j,  0.0308-1.1464e-01j,\n",
      "         0.0298-7.5746e-02j,  0.0277-5.1892e-02j,  0.0256-3.6700e-02j,\n",
      "         0.0222-2.4451e-02j], device='cuda:0') tensor([-0.3419,  0.5125], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "NTRAIN = 200000\n",
    "NTEST = 50000\n",
    "cachedataset_prefix = \"antiglitch_cvnn_dataset_\"\n",
    "\n",
    "class GlitchDataset(torch.utils.data.TensorDataset):\n",
    "\tdef __init__(self, ifos, ml_models, datadict, glitches, distributions, size):\n",
    "\t\tself.ifos = ifos\n",
    "\t\tself.ml_models = ml_models\n",
    "\t\tself.datadict = datadict\n",
    "\t\tself.glitches = glitches\n",
    "\t\tself.distributions = distributions\n",
    "\t\tself.size = size\n",
    "\t\tself.x_arr = []\n",
    "\t\tself.y_arr = []\n",
    "\t\t# create dataset\n",
    "\t\tif cachedataset_prefix + str(size) + '.npz' in os.listdir(datadir):\n",
    "\t\t\tdata = np.load(datadir + cachedataset_prefix + str(size) + '.npz')\n",
    "\t\t\tself.x_arr = data['x_arr']\n",
    "\t\t\tself.y_arr = data['y_arr']\n",
    "\t\telse:\n",
    "\t\t\tfor i in range(size):\n",
    "\t\t\t\tx, y = get_snip(ifos, ml_models)\n",
    "\t\t\t\tself.x_arr.append(x)\n",
    "\t\t\t\tself.y_arr.append(y)\n",
    "\t\t\tself.x_arr = np.array(self.x_arr)\n",
    "\t\t\tself.y_arr = np.array(self.y_arr)\n",
    "\t\t\t# normalize x_arr\n",
    "\t\t\treal_parts = np.real(self.x_arr)\n",
    "\t\t\timag_parts = np.imag(self.x_arr)\n",
    "\t\t\t\n",
    "\t\t\tmean_real = np.mean(real_parts)\n",
    "\t\t\tstd_real = np.std(real_parts)\n",
    "\t\t\t\n",
    "\t\t\tmean_imag = np.mean(imag_parts)\n",
    "\t\t\tstd_imag = np.std(imag_parts)\n",
    "\t\t\t\n",
    "\t\t\tnormalized_real = (real_parts - mean_real) / std_real\n",
    "\t\t\tnormalized_imag = (imag_parts - mean_imag) / std_imag\n",
    "\t\t\t\n",
    "\t\t\tself.x_arr = normalized_real + 1j * normalized_imag\n",
    "\t\t\t# normalize y_arr\n",
    "\t\t\ty1_mean = self.y_arr[:,0].mean()\n",
    "\t\t\ty1_std = self.y_arr[:,0].std()\n",
    "\t\t\ty2_mean = self.y_arr[:,1].mean()\n",
    "\t\t\ty2_std = self.y_arr[:,1].std()\n",
    "\t\t\tself.y_arr[:,0] = (self.y_arr[:,0] - y1_mean)/y1_std\n",
    "\t\t\tself.y_arr[:,1] = (self.y_arr[:,1] - y2_mean)/y2_std\n",
    "\t\t\tnp.savez(datadir + cachedataset_prefix + str(size) + '.npz', x_arr=self.x_arr, y_arr=self.y_arr)\n",
    "\t\t# convert to tensors\n",
    "\t\tself.x_arr = torch.tensor(self.x_arr, dtype=torch.complex64, device=device)\n",
    "\t\tself.y_arr = torch.tensor(self.y_arr, dtype=torch.float32, device=device)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.size\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn self.x_arr[idx], self.y_arr[idx]\n",
    "\n",
    "train_data = GlitchDataset(ifos, ml_models, datadict, glitches, distributions, NTRAIN)\n",
    "test_data = GlitchDataset(ifos, ml_models, datadict, glitches, distributions, NTEST)\n",
    "testitem = train_data.__getitem__(0)\n",
    "print(testitem[0][1:20], testitem[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These were cells used for preprocessing the data before I was told I could use the Snippet class to generate - whoops.\n",
    "# # Now flatten to x and y data where x is glitched data and y is the [f0, gbw]\n",
    "\n",
    "# x = []\n",
    "# y = []\n",
    "# for ifo in ifos:\n",
    "# \tfor key in keys:\n",
    "# \t\tfor num in datadict[ifo][key]:\n",
    "# \t\t\ttry:\n",
    "# \t\t\t\tx.append(glitches[ifo][key][num]['data'])\n",
    "# \t\t\t\ty.append([glitches[ifo][key][num]['f0'], glitches[ifo][key][num]['gbw']])\n",
    "# \t\t\texcept:\n",
    "# \t\t\t\tpass\n",
    "# x = np.array(x)\n",
    "# y = np.array(y)\n",
    "# type(x), x.shape, type(y), y.shape\n",
    "# # Split data into training and testing\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Convert to torch tensors\n",
    "# x_train = torch.tensor(x_train, dtype=torch.complex64)\n",
    "# x_test = torch.tensor(x_test, dtype=torch.complex64)\n",
    "# y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "# y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "# # Normalize data\n",
    "# x_train = (x_train - x_train.mean()) / x_train.std()\n",
    "# x_test = (x_test - x_test.mean()) / x_test.std()\n",
    "# y_train = (y_train - y_train.mean()) / y_train.std()\n",
    "# y_test = (y_test - y_test.mean()) / y_test.std()\n",
    "# # put data on device\n",
    "# x_train = x_train.to(device)\n",
    "# x_test = x_test.to(device)\n",
    "# y_train = y_train.to(device)\n",
    "# y_test = y_test.to(device)\n",
    "# # Print out a description and sample of the data\n",
    "# print(x_train.shape, x_train.dtype, x_train[0])\n",
    "# print(x_test.shape, x_test.dtype, x_test[0])\n",
    "# print(y_train.shape, y_train.dtype, y_train[0])\n",
    "# print(y_test.shape, y_test.dtype, y_test[0])\n",
    "# train_data = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "# test_data = torch.utils.data.TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 513])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load into DataLoader\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=1024, shuffle=True, drop_last=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1024, shuffle=False, drop_last=False)\n",
    "list(train_loader.__iter__())[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexValuedNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexValuedNN, self).__init__()\n",
    "\n",
    "        # Complex Conv Layer 1\n",
    "        self.conv1 = cvtorch.nn.CVConv1d(1, 32, 3, padding=\"same\")\n",
    "        self.activation1 = cvtorch.nn.zReLU()\n",
    "        # self.bn1 = cvtorch.nn.CVBatchNorm1d(32)\n",
    "        # self.dropout1 = cvtorch.nn.CVDropout(0.25)\n",
    "\n",
    "        # Complex Conv Layer 2\n",
    "        self.conv2 = cvtorch.nn.CVConv1d(32, 64, 5, padding=\"same\")\n",
    "        self.activation2 = cvtorch.nn.zReLU()\n",
    "        # self.bn2 = cvtorch.nn.CVBatchNorm1d(64)\n",
    "        # self.dropout2 = cvtorch.nn.CVDropout(0.25)\n",
    "\n",
    "        # Fully connected Layer 1\n",
    "        self.fc1 = cvtorch.nn.CVLinear(64 * 513, 128)\n",
    "        self.activation3 = cvtorch.nn.zReLU()\n",
    "        # self.bn3 = cvtorch.nn.CVBatchNorm1d(1024)\n",
    "        # self.dropout3 = cvtorch.nn.CVDropout(0.25)\n",
    "\n",
    "        # Fully connected Layer 2\n",
    "        # self.fc2 = cvtorch.nn.CVLinear(1024, 1024)\n",
    "        # self.activation4 = cvtorch.nn.zReLU()\n",
    "        # self.bn4 = cvtorch.nn.CVBatchNorm1d(1024)\n",
    "        # self.dropout4 = cvtorch.nn.CVDropout(0.25)\n",
    "        # Fully connected Layer 2\n",
    "        self.fc3 = cvtorch.nn.CVLinear(128, 32)\n",
    "        self.activation5 = cvtorch.nn.zReLU()\n",
    "        \n",
    "        # self.bn5 = cvtorch.nn.CVBatchNorm1d(32)\n",
    "        # self.dropout5 = cvtorch.nn.CVDropout(0.25)\n",
    "        \n",
    "        # Output Layer (real-valued)\n",
    "        self.output_layer = nn.Linear(32 * 2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add channel dimension for convolution\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        # Conv Layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation1(x)\n",
    "        # x = self.bn1(x)\n",
    "        # x = self.dropout1(x)\n",
    "\n",
    "        # # # Conv Layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation2(x)\n",
    "        # x = self.bn2(x)\n",
    "        # x = self.dropout2(x)\n",
    "\n",
    "        # Flatten the tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully Connected Layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation3(x)\n",
    "        # x = self.bn3(x)\n",
    "        # x = self.dropout3(x)\n",
    "\n",
    "        # Fully Connected Layer 2\n",
    "        # x = self.fc2(x)\n",
    "        # x = self.activation4(x)\n",
    "        # x = self.bn4(x)\n",
    "        # x = self.dropout4(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.activation5(x)\n",
    "        # x = self.bn5(x)\n",
    "        # x = self.dropout5(x)\n",
    "        # Transform complex valued output for the final real-valued layer\n",
    "        real_x = torch.cat((x.real, x.imag), dim=1)  # Merge real and imag parts\n",
    "\n",
    "        # Output Layer\n",
    "        out = self.output_layer(real_x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComplexValuedNN(\n",
      "  (conv1): CVConv1d(\n",
      "    (conv_r): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=same)\n",
      "    (conv_i): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  )\n",
      "  (activation1): zReLU()\n",
      "  (conv2): CVConv1d(\n",
      "    (conv_r): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=same)\n",
      "    (conv_i): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=same)\n",
      "  )\n",
      "  (activation2): zReLU()\n",
      "  (fc1): CVLinear(\n",
      "    (linear_r): Linear(in_features=32832, out_features=128, bias=False)\n",
      "    (linear_i): Linear(in_features=32832, out_features=128, bias=False)\n",
      "  )\n",
      "  (activation3): zReLU()\n",
      "  (fc3): CVLinear(\n",
      "    (linear_r): Linear(in_features=128, out_features=32, bias=False)\n",
      "    (linear_i): Linear(in_features=128, out_features=32, bias=False)\n",
      "  )\n",
      "  (activation5): zReLU()\n",
      "  (output_layer): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "Number of parameters: 8434178\n"
     ]
    }
   ],
   "source": [
    "# Define the input and output dimensions based on the dataset\n",
    "input_dim = 513  # Number of input features\n",
    "output_dim = 2  # Number of output units (as specified in the dataset)\n",
    "\n",
    "# Create an instance of the network\n",
    "model = ComplexValuedNN()\n",
    "\n",
    "# put model on device\n",
    "model.to(device)\n",
    "\n",
    "# model = torch.compile(model)\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model if it exists\n",
    "model_path = 'complex_nn_model.pth'\n",
    "try:\n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        print('Model loaded')\n",
    "except:\n",
    "    print('Model not loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=20, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Training Loss: 0.6597, Validation Loss: 1.3385\n",
      "Epoch [2/200], Training Loss: 0.3960, Validation Loss: 1.5106\n",
      "Epoch [3/200], Training Loss: 0.3573, Validation Loss: 1.8187\n",
      "Epoch [4/200], Training Loss: 0.4524, Validation Loss: 1.7916\n",
      "Epoch [5/200], Training Loss: 0.3311, Validation Loss: 1.4534\n",
      "Epoch [6/200], Training Loss: 0.3834, Validation Loss: 1.7292\n",
      "Epoch [7/200], Training Loss: 0.3286, Validation Loss: 2.0100\n",
      "Epoch [8/200], Training Loss: 0.2803, Validation Loss: 2.1725\n",
      "Epoch [9/200], Training Loss: 0.3617, Validation Loss: 2.1758\n",
      "Epoch [10/200], Training Loss: 0.2700, Validation Loss: 1.9132\n",
      "Epoch [11/200], Training Loss: 0.2453, Validation Loss: 1.8925\n",
      "Epoch [12/200], Training Loss: 0.1639, Validation Loss: 0.3369\n",
      "Epoch [13/200], Training Loss: 0.2377, Validation Loss: 0.3165\n",
      "Epoch [14/200], Training Loss: 0.2353, Validation Loss: 0.2952\n",
      "Epoch [15/200], Training Loss: 0.1965, Validation Loss: 0.2633\n",
      "Epoch [16/200], Training Loss: 0.1690, Validation Loss: 0.2594\n",
      "Epoch [17/200], Training Loss: 0.1480, Validation Loss: 0.3037\n",
      "Epoch [18/200], Training Loss: 0.2221, Validation Loss: 0.2288\n",
      "Epoch [19/200], Training Loss: 0.1685, Validation Loss: 0.2401\n",
      "Epoch [20/200], Training Loss: 0.1193, Validation Loss: 0.2278\n",
      "Epoch [21/200], Training Loss: 0.1395, Validation Loss: 0.3007\n",
      "Epoch [22/200], Training Loss: 0.2016, Validation Loss: 0.2254\n",
      "Epoch [23/200], Training Loss: 0.1261, Validation Loss: 0.2253\n",
      "Epoch [24/200], Training Loss: 0.2099, Validation Loss: 0.2748\n",
      "Epoch [25/200], Training Loss: 0.1247, Validation Loss: 0.2102\n",
      "Epoch [26/200], Training Loss: 0.1842, Validation Loss: 0.2002\n",
      "Epoch [27/200], Training Loss: 0.2275, Validation Loss: 0.2658\n",
      "Epoch [28/200], Training Loss: 0.1810, Validation Loss: 0.2735\n",
      "Epoch [29/200], Training Loss: 0.1732, Validation Loss: 0.3048\n",
      "Epoch [30/200], Training Loss: 0.1825, Validation Loss: 0.2331\n",
      "Epoch [31/200], Training Loss: 0.1217, Validation Loss: 0.2412\n",
      "Epoch [32/200], Training Loss: 0.1692, Validation Loss: 0.2048\n",
      "Epoch [33/200], Training Loss: 0.1207, Validation Loss: 0.1927\n",
      "Epoch [34/200], Training Loss: 0.1005, Validation Loss: 0.2073\n",
      "Epoch [35/200], Training Loss: 0.1314, Validation Loss: 0.3719\n",
      "Epoch [36/200], Training Loss: 0.1304, Validation Loss: 0.1797\n",
      "Epoch [37/200], Training Loss: 0.1021, Validation Loss: 0.1945\n",
      "Epoch [38/200], Training Loss: 0.1578, Validation Loss: 0.1766\n",
      "Epoch [39/200], Training Loss: 0.1698, Validation Loss: 0.1794\n",
      "Epoch [40/200], Training Loss: 0.1567, Validation Loss: 0.1710\n",
      "Epoch [41/200], Training Loss: 0.1021, Validation Loss: 0.2121\n",
      "Epoch [42/200], Training Loss: 0.1088, Validation Loss: 0.2079\n",
      "Epoch [43/200], Training Loss: 0.1290, Validation Loss: 0.1726\n",
      "Epoch [44/200], Training Loss: 0.1498, Validation Loss: 0.2024\n",
      "Epoch [45/200], Training Loss: 0.1348, Validation Loss: 0.2172\n",
      "Epoch [46/200], Training Loss: 0.0842, Validation Loss: 0.1665\n",
      "Epoch [47/200], Training Loss: 0.0934, Validation Loss: 0.1748\n",
      "Epoch [48/200], Training Loss: 0.1527, Validation Loss: 0.1617\n",
      "Epoch [49/200], Training Loss: 0.0848, Validation Loss: 0.1838\n",
      "Epoch [50/200], Training Loss: 0.0916, Validation Loss: 0.1676\n",
      "Epoch [51/200], Training Loss: 0.1156, Validation Loss: 0.1575\n",
      "Epoch [52/200], Training Loss: 0.1285, Validation Loss: 0.1854\n",
      "Epoch [53/200], Training Loss: 0.0818, Validation Loss: 0.1693\n",
      "Epoch [54/200], Training Loss: 0.0916, Validation Loss: 0.1670\n",
      "Epoch [55/200], Training Loss: 0.0766, Validation Loss: 0.1638\n",
      "Epoch [56/200], Training Loss: 0.0644, Validation Loss: 0.1830\n",
      "Epoch [57/200], Training Loss: 0.0773, Validation Loss: 0.1710\n",
      "Epoch [58/200], Training Loss: 0.0914, Validation Loss: 0.1902\n",
      "Epoch [59/200], Training Loss: 0.1333, Validation Loss: 0.1661\n",
      "Epoch [60/200], Training Loss: 0.0642, Validation Loss: 0.1737\n",
      "Epoch [61/200], Training Loss: 0.0961, Validation Loss: 0.2282\n",
      "Epoch [62/200], Training Loss: 0.1118, Validation Loss: 0.1678\n",
      "Epoch [63/200], Training Loss: 0.0691, Validation Loss: 0.1756\n",
      "Epoch [64/200], Training Loss: 0.1068, Validation Loss: 0.1853\n",
      "Epoch [65/200], Training Loss: 0.0725, Validation Loss: 0.1958\n",
      "Epoch [66/200], Training Loss: 0.0765, Validation Loss: 0.2205\n",
      "Epoch [67/200], Training Loss: 0.0621, Validation Loss: 0.1951\n",
      "Epoch [68/200], Training Loss: 0.0858, Validation Loss: 0.2057\n",
      "Epoch [69/200], Training Loss: 0.0920, Validation Loss: 0.1827\n",
      "Epoch [70/200], Training Loss: 0.0910, Validation Loss: 0.1879\n",
      "Epoch [71/200], Training Loss: 0.0904, Validation Loss: 0.2376\n",
      "Epoch [72/200], Training Loss: 0.0634, Validation Loss: 0.2026\n",
      "Epoch [73/200], Training Loss: 0.0563, Validation Loss: 0.1888\n",
      "Epoch [74/200], Training Loss: 0.0601, Validation Loss: 0.1666\n",
      "Epoch [75/200], Training Loss: 0.1156, Validation Loss: 0.1724\n",
      "Epoch [76/200], Training Loss: 0.1281, Validation Loss: 0.1827\n",
      "Epoch [77/200], Training Loss: 0.0807, Validation Loss: 0.1904\n",
      "Epoch [78/200], Training Loss: 0.0837, Validation Loss: 0.1733\n",
      "Epoch [79/200], Training Loss: 0.1335, Validation Loss: 0.1779\n",
      "Epoch [80/200], Training Loss: 0.0562, Validation Loss: 0.1670\n",
      "Epoch [81/200], Training Loss: 0.0744, Validation Loss: 0.1944\n",
      "Epoch [82/200], Training Loss: 0.0654, Validation Loss: 0.1835\n",
      "Epoch [83/200], Training Loss: 0.0716, Validation Loss: 0.1783\n",
      "Epoch [84/200], Training Loss: 0.0697, Validation Loss: 0.1824\n",
      "Epoch [85/200], Training Loss: 0.0811, Validation Loss: 0.1873\n",
      "Epoch [86/200], Training Loss: 0.1132, Validation Loss: 0.1916\n",
      "Epoch [87/200], Training Loss: 0.0454, Validation Loss: 0.1946\n",
      "Epoch [88/200], Training Loss: 0.1256, Validation Loss: 0.2017\n",
      "Epoch [89/200], Training Loss: 0.0635, Validation Loss: 0.1979\n",
      "Epoch [90/200], Training Loss: 0.0953, Validation Loss: 0.1824\n",
      "Epoch [91/200], Training Loss: 0.1115, Validation Loss: 0.1836\n",
      "Epoch [92/200], Training Loss: 0.1101, Validation Loss: 0.2086\n",
      "Epoch [93/200], Training Loss: 0.0602, Validation Loss: 0.1831\n",
      "Epoch [94/200], Training Loss: 0.0774, Validation Loss: 0.1831\n",
      "Epoch [95/200], Training Loss: 0.0533, Validation Loss: 0.1833\n",
      "Epoch [96/200], Training Loss: 0.0693, Validation Loss: 0.1831\n",
      "Epoch [97/200], Training Loss: 0.0674, Validation Loss: 0.1837\n",
      "Epoch [98/200], Training Loss: 0.0802, Validation Loss: 0.1825\n",
      "Epoch [99/200], Training Loss: 0.0746, Validation Loss: 0.1817\n",
      "Epoch [100/200], Training Loss: 0.0694, Validation Loss: 0.1804\n",
      "Epoch [101/200], Training Loss: 0.1005, Validation Loss: 0.1801\n",
      "Epoch [102/200], Training Loss: 0.0563, Validation Loss: 0.1802\n",
      "Epoch [103/200], Training Loss: 0.0863, Validation Loss: 0.1884\n",
      "Epoch [104/200], Training Loss: 0.0652, Validation Loss: 0.1895\n",
      "Epoch [105/200], Training Loss: 0.0626, Validation Loss: 0.1906\n",
      "Epoch [106/200], Training Loss: 0.0696, Validation Loss: 0.1911\n",
      "Epoch [107/200], Training Loss: 0.0533, Validation Loss: 0.1922\n",
      "Epoch [108/200], Training Loss: 0.0738, Validation Loss: 0.1935\n",
      "Epoch [109/200], Training Loss: 0.0617, Validation Loss: 0.1936\n",
      "Epoch [110/200], Training Loss: 0.0594, Validation Loss: 0.1940\n",
      "Epoch [111/200], Training Loss: 0.0683, Validation Loss: 0.1946\n",
      "Epoch [112/200], Training Loss: 0.0941, Validation Loss: 0.1889\n",
      "Epoch [113/200], Training Loss: 0.1179, Validation Loss: 0.1891\n",
      "Epoch [114/200], Training Loss: 0.0587, Validation Loss: 0.1782\n",
      "Epoch [115/200], Training Loss: 0.0759, Validation Loss: 0.1786\n",
      "Epoch [116/200], Training Loss: 0.0582, Validation Loss: 0.1788\n",
      "Epoch [117/200], Training Loss: 0.0737, Validation Loss: 0.1867\n",
      "Epoch [118/200], Training Loss: 0.0695, Validation Loss: 0.1878\n",
      "Epoch [119/200], Training Loss: 0.0435, Validation Loss: 0.1881\n",
      "Epoch [120/200], Training Loss: 0.0795, Validation Loss: 0.1884\n",
      "Epoch [121/200], Training Loss: 0.0785, Validation Loss: 0.1887\n",
      "Epoch [122/200], Training Loss: 0.0649, Validation Loss: 0.1891\n",
      "Epoch [123/200], Training Loss: 0.0897, Validation Loss: 0.1890\n",
      "Epoch [124/200], Training Loss: 0.0613, Validation Loss: 0.1894\n",
      "Epoch [125/200], Training Loss: 0.0920, Validation Loss: 0.1897\n",
      "Epoch [126/200], Training Loss: 0.0549, Validation Loss: 0.1898\n",
      "Epoch [127/200], Training Loss: 0.0477, Validation Loss: 0.1891\n",
      "Epoch [128/200], Training Loss: 0.1097, Validation Loss: 0.1803\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blen/home/xangma/repos/antiglitch/notebooks/cvnn_playground.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Test the model\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[1;32m    268\u001b[0m     tensors,\n\u001b[1;32m    269\u001b[0m     grad_tensors_,\n\u001b[1;32m    270\u001b[0m     retain_graph,\n\u001b[1;32m    271\u001b[0m     create_graph,\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    275\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/antiglitch/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m         t_outputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    746\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            total_loss += criterion(outputs, targets).item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}, Validation Loss: {total_loss/len(test_loader):.4f}\")\n",
    "    \n",
    "    # update the learning rate\n",
    "    scheduler.step(total_loss/len(test_loader))\n",
    "\n",
    "    writer.add_scalars(\"Loss\", {\"Train\": loss.item(), \"Validation\": total_loss/len(test_loader)}, epoch)\n",
    "    writer.add_scalar(\"Learning rate\", optimizer.param_groups[0]['lr'], epoch)\n",
    "    \n",
    "    # save the model every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        \n",
    "writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [ 0.11066131 -0.82846916]\n",
      "Target: [ 0.2531437  -0.76916826]\n",
      "\n",
      "\n",
      "Prediction: [ 2.4060965 -0.9313154]\n",
      "Target: [ 3.581295  -0.7205246]\n",
      "\n",
      "\n",
      "Prediction: [-0.28759983  0.22473028]\n",
      "Target: [-0.38967282  0.13980909]\n",
      "\n",
      "\n",
      "Prediction: [-0.33022934  1.7921206 ]\n",
      "Target: [-0.3106424  2.2877557]\n",
      "\n",
      "\n",
      "Prediction: [-0.47873828 -0.14912502]\n",
      "Target: [-0.4583077  -0.07159861]\n",
      "\n",
      "\n",
      "Prediction: [-0.62700087  2.3018472 ]\n",
      "Target: [-0.51930755  2.738316  ]\n",
      "\n",
      "\n",
      "Prediction: [-0.5536676  0.7405968]\n",
      "Target: [-0.49190506  0.8744962 ]\n",
      "\n",
      "\n",
      "Prediction: [-0.5106758  1.113033 ]\n",
      "Target: [-0.35114104  1.5091016 ]\n",
      "\n",
      "\n",
      "Prediction: [-0.38125008 -0.7338947 ]\n",
      "Target: [-0.5385673  -0.79498357]\n",
      "\n",
      "\n",
      "Prediction: [-0.39388457  2.105945  ]\n",
      "Target: [-0.52113533  1.5611671 ]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if type(outputs) == torch.Tensor:\n",
    "    outputs = outputs.cpu().detach().numpy()\n",
    "if type(targets) == torch.Tensor:\n",
    "    targets = targets.cpu().detach().numpy()\n",
    "for i in range(0,10):\n",
    "    print(f\"Prediction: {outputs[i]}\")\n",
    "    print(f\"Target: {targets[i]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_path = 'complex_nn_model.pth'\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = ComplexValuedNN(input_dim, output_dim)\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "antiglitch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
